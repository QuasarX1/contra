# SPDX-FileCopyrightText: 2024-present Christopher Rowe <chris.rowe19@outlook.com>
#
# SPDX-License-Identifier: None
import datetime
from typing import Any, Union, Collection, List, Dict
import asyncio
from concurrent.futures import ThreadPoolExecutor
from functools import singledispatchmethod

import numpy as np
import h5py as h5
from unyt import unyt_quantity
from QuasarCode import Console
from QuasarCode.Data import VersionInfomation
from QuasarCode.Tools import Struct, TypedAutoProperty, NullableTypedAutoProperty, TypeShield, NestedTypeShield

from .._ArrayReorder import ArrayReorder
from .._ParticleType import ParticleType
from ._SnapshotBase import SnapshotBase
from ._SnapshotSWIFT import SnapshotSWIFT
from ._SnapshotEAGLE import SnapshotEAGLE
from ._CatalogueBase import CatalogueBase
from ._CatalogueSOAP import CatalogueSOAP
from ._CatalogueSUBFIND import CatalogueSUBFIND

class HeaderDataset(Struct):
    """
    version

    date

    target_snapshot

    target_catalogue_membership_file

    target_catalogue_properties_file

    simulation_type

    redshift

    N_searched_snapshots

    output_file

    has_gas

    has_stars

    has_black_holes

    has_dark_matter

    has_statistics
    """

    version = TypedAutoProperty[VersionInfomation](TypeShield(VersionInfomation),
                doc = "Contra version number.")
    date = TypedAutoProperty[datetime.date](TypeShield(datetime.date),
                doc = "Date of execution (start of program).")
    target_snapshot = TypedAutoProperty[str](TypeShield(str),
                doc = "Snapshot file defining positions of target particles.")
    target_catalogue_membership_file = TypedAutoProperty[str](TypeShield(str),
                doc = "Catalogue membership file corresponding to the target snapshot.")
    target_catalogue_properties_file = TypedAutoProperty[str](TypeShield(str),
                doc = "Catalogue properties file corresponding to the target snapshot.")
    simulation_type = TypedAutoProperty[str](TypeShield(str),
                doc = "Type of source simulation data.")
    redshift = TypedAutoProperty[float](TypeShield(float),
                doc = "Redshift of the target particle distribution.")
    N_searched_snapshots = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of snapshots searched.")
#    searched_files = TypedAutoProperty[List[str]](TypeShield(List[str]),
#                doc = "List of catalogue files searched.")
    output_file = TypedAutoProperty[str](TypeShield(str),
                doc = "This output file location as generated by Contra.")
    has_gas = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for gas particles.")
    has_stars = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for star particles.")
    has_black_holes = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for black hole particles.")
    has_dark_matter = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for dark matter particles.")
    has_statistics = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains statistics for each snapshot searched.")



class ParticleTypeDataset(Struct):#TODO: data is only float/int 32 bit precision!
    """
    particle_type

    redshifts

    halo_ids

    halo_masses

    halo_masses_scaled

    locations_pre_ejection
    """

    particle_type = TypedAutoProperty[ParticleType](TypeShield(ParticleType),
                doc = "Particle type.")
    redshifts = TypedAutoProperty[Collection[float]](NestedTypeShield(np.ndarray, np.float64),
                doc = "Redshift of last halo membership.")
    halo_ids = TypedAutoProperty[Collection[int]](NestedTypeShield(np.ndarray, np.int64),
                doc = "ID of last halo.")
    halo_masses = TypedAutoProperty[Collection[float]](NestedTypeShield(np.ndarray, np.float64),#TODO: make unyt
                doc = "Mass of last halo.")
    halo_masses_scaled = TypedAutoProperty[Collection[float]](NestedTypeShield(np.ndarray, np.float64),
                doc = "Mass of last halo as a fraction of the L_* mass at that redshift.")
    positions_pre_ejection = TypedAutoProperty[Collection[Collection[float]]](NestedTypeShield(np.ndarray, np.ndarray, np.float64),#TODO: make unyt
                doc = "Particle coordinates prior to final ejection.")



class CheckpointData(ParticleTypeDataset):
    """
    last_complete_snap_index

    missing_particles_mask
    """

    last_complete_snap_index = TypedAutoProperty[int](TypeShield(int),
                doc = "Index of last .")
    missing_particles_mask = TypedAutoProperty[np.ndarray](NestedTypeShield(np.ndarray, np.bool_),
                doc = "Boolean mask for particles that have not yet been found in haloes.")



class SnapshotStatsDataset(Struct):
    """
    snapshot_filepath

    catalogue_membership_filepath

    catalogue_properties_filepath

    redshift

    N_particles

    N_particles_(gas | star | black_hole | dark_matter)

    N_haloes

    N_haloes_top_level

    N_halo_children

    N_halo_decendants

    N_halo_particles

    N_halo_particles_(gas | star | black_hole | dark_matter)

    N_particles_matched

    N_particles_matched_(gas | star | black_hole | dark_matter)

    particle_total_volume

    halo_particle_total_volume

    particles_matched_total_volume

    particles_matched_total_volume_(gas | star | black_hole | dark_matter)
    """

    snapshot_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of snapshot file.")
    catalogue_membership_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of catalogue file containing particle membership information.")
    catalogue_properties_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of catalogue file containing halo properties.")
    catalogue_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of catalogue file.")
    redshift = TypedAutoProperty[float](TypeShield(float),
                doc = "Snapshot redshift.")
    N_particles = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of particles in the snapshot.")
    N_particles_gas = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of gas particles in the snapshot.")
    N_particles_star = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of star particles in the snapshot.")
    N_particles_black_hole = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of black hole particles in the snapshot.")
    N_particles_dark_matter = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of dark matter particles in the snapshot.")
    N_haloes = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of haloes.")
    N_haloes_top_level = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of haloes with no parent.")
    N_halo_children = NullableTypedAutoProperty[Collection[int]](NestedTypeShield(np.ndarray, np.int64),
                doc = "Number of direct children of each halo.")
    N_halo_decendants = NullableTypedAutoProperty[Collection[int]](NestedTypeShield(np.ndarray, np.int64),
                doc = "Total number of decendants of each halo.")
    N_halo_particles = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of particles in haloes.")
    N_halo_particles_gas = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of gas particles.")
    N_halo_particles_star = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of star particles.")
    N_halo_particles_black_hole = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of black hole particles.")
    N_halo_particles_dark_matter = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of dark matter particles.")
    N_particles_matched = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched particles in this snapshot.")
    N_particles_matched_gas = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched gas particles in this snapshot.")
    N_particles_matched_star = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched star particles in this snapshot.")
    N_particles_matched_black_hole = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched black_hole particles in this snapshot.")
    N_particles_matched_dark_matter = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched dark_matter particles in this snapshot.")
    particle_total_volume = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of all particles in the snapshot.")
    halo_particle_total_volume = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume particles in haloes in the snapshot.")
    particles_matched_total_volume = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched particles in the snapshot.")
    particles_matched_total_volume_gas = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched gas particles in the snapshot.")
    particles_matched_total_volume_star = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched star particles in the snapshot.")
    particles_matched_total_volume_black_hole = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched black hole particles in the snapshot.")
    particles_matched_total_volume_dark_matter = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched dark matter particles in the snapshot.")
    
    def __str__(self):
        return f"""\
redshift:                                {self.redshift}
snapshot file:                           {self.snapshot_filepath}
catalogue membership file:               {self.catalogue_membership_filepath}
catalogue properties file:               {self.catalogue_properties_filepath}
number of particles:                     {self.N_particles}
number of particles in haloes:           {self.N_halo_particles if self.N_halo_particles is not None else "---"}
number of particles newley matched:      {self.N_particles_matched}
total particle volume:                   {self.particle_total_volume if self.particle_total_volume is not None else "---"}
number of haloes:                        {self.N_haloes}
number of top level haloes:              {self.N_haloes_top_level if self.N_haloes_top_level is not None else "---"}
total volume of particles in haloes:     {self.halo_particle_total_volume if self.halo_particle_total_volume is not None else "---"}
total volume of particles newly matched: {self.particles_matched_total_volume if self.particles_matched_total_volume is not None else "---"}\
""" + (f"""
gas:
    number of particles:                     {self.N_particles_gas}
    number of particles in haloes:           {self.N_halo_particles_gas if self.N_halo_particles_gas is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_gas}
    total volume of particles newly matched: {self.particles_matched_total_volume_gas if self.particles_matched_total_volume_gas is not None else "---"}\
""" if self.N_particles_gas is not None        else "gas: -----------") + (f"""
stars:
    number of particles:                     {self.N_particles_star}
    number of particles in haloes:           {self.N_halo_particles_star if self.N_halo_particles_star is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_star}
    total volume of particles newly matched: {self.particles_matched_total_volume_star if self.particles_matched_total_volume_star is not None else "---"}\
""" if self.N_particles_star is not None       else "stars: ---------") + (f"""
black holes:
    number of particles:                     {self.N_particles_black_hole}
    number of particles in haloes:           {self.N_halo_particles_black_hole if self.N_halo_particles_black_hole is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_black_hole}
    total volume of particles newly matched: {self.particles_matched_total_volume_black_hole if self.particles_matched_total_volume_black_hole is not None else "---"}\
""" if self.N_particles_black_hole is not None else "black holes: ---") + (f"""
dark mater:
    number of particles:                     {self.N_particles_dark_matter}
    number of particles in haloes:           {self.N_halo_particles_dark_matter if self.N_halo_particles_dark_matter is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_dark_matter}
    total volume of particles newly matched: {self.particles_matched_total_volume_dark_matter if self.particles_matched_total_volume_dark_matter is not None else "---"}\
""" if self.N_particles_dark_matter is not None else "dark mater: ---")
    
    @staticmethod
    def initialise_partial(snapshot: SnapshotBase, catalogue: CatalogueBase, minimal: bool = False) -> "SnapshotStatsDataset":
        data =  SnapshotStatsDataset()

        data.snapshot_filepath = snapshot.filepath
        data.catalogue_membership_filepath = catalogue.membership_filepath
        data.catalogue_properties_filepath = catalogue.properties_filepath
        data.redshift = catalogue.z
        data.N_particles = sum([snapshot.number_of_particles(p) for p in ParticleType.get_all()])
        for part_type in ParticleType.get_all():
            setattr(data, f"N_particles_{part_type.name.replace(' ', '_')}", snapshot.number_of_particles(part_type))
        data.N_haloes = len(catalogue)
        if data.N_haloes > 0:
            data.N_halo_children = catalogue.number_of_children
            data.N_halo_decendants = catalogue.number_of_decendants

        if not minimal:
            data.particle_total_volume = sum(
                [
                    snapshot.get_volumes(p).to("Mpc**3").sum()
                    for p
                    in ParticleType.get_all()
                ],
                start = unyt_quantity(0.0, units = "Mpc**3")
            )

            data.N_haloes_top_level = int((catalogue.get_halo_parent_IDs() == -1).sum())

            data.N_halo_particles_gas = len(catalogue.get_particle_IDs(ParticleType.gas))
            data.N_halo_particles_star = len(catalogue.get_particle_IDs(ParticleType.star))
            data.N_halo_particles_black_hole = len(catalogue.get_particle_IDs(ParticleType.black_hole))
            data.N_halo_particles_dark_matter = len(catalogue.get_particle_IDs(ParticleType.dark_matter))

            data.N_halo_particles = sum([len(catalogue.get_particle_IDs(p)) for p in ParticleType.get_all()])
            data.halo_particle_total_volume = sum([ArrayReorder.create(snapshot.get_IDs(p), catalogue.get_particle_IDs(p))(snapshot.get_volumes(p).to("Mpc**3")).sum() for p in ParticleType.get_all()], start = unyt_quantity(0.0, units = "Mpc**3"))

        return data

    @staticmethod
    async def initialise_partial_async(snapshot: SnapshotBase, catalogue: CatalogueBase, minimal: bool = False) -> "SnapshotStatsDataset":#TODO:
        data =  SnapshotStatsDataset()

        data.snapshot_filepath = snapshot.filepath
        data.catalogue_membership_filepath = catalogue.membership_filepath
        data.catalogue_properties_filepath = catalogue.properties_filepath
        data.redshift = catalogue.z
        data.N_particles = sum([snapshot.number_of_particles(p) for p in ParticleType.get_all()])
        for part_type in ParticleType.get_all():
            setattr(data, f"N_particles_{part_type.name.replace(' ', '_')}", snapshot.number_of_particles(part_type))
        data.N_haloes = len(catalogue)
        if data.N_haloes > 0:
            data.N_halo_children = catalogue.number_of_children
            data.N_halo_decendants = catalogue.number_of_decendants

        if not minimal:

            # Async operation functions

            async def calc__particle_total_volume():
                data.particle_total_volume = sum(
                    [
                        (smoothing_lengths.to("Mpc")**3).sum()
                        for smoothing_lengths
                        in await asyncio.gather(*[snapshot.get_smoothing_lengths_async(p) for p in ParticleType.get_all()])
                    ],
                    start = unyt_quantity(0.0, units = "Mpc**3")
                ) * (np.pi * 4/3)

            async def calc__N_haloes_top_level():
                data.N_haloes_top_level = int((await catalogue.get_halo_parent_IDs_async() == -1).sum())

            async def calc__N_halo_particles_of_type(part_type: ParticleType):
                return len(await catalogue.get_particle_IDs_async(part_type))
            async def calc__N_halo_particles():
                (
                    data.N_halo_particles_gas,
                    data.N_halo_particles_star,
                    data.N_halo_particles_black_hole,
                    data.N_halo_particles_dark_matter,
                ) = await asyncio.gather(
                    calc__N_halo_particles_of_type(ParticleType.gas),
                    calc__N_halo_particles_of_type(ParticleType.star),
                    calc__N_halo_particles_of_type(ParticleType.black_hole),
                    calc__N_halo_particles_of_type(ParticleType.dark_matter)
                )

            # Run all async functions
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                future = loop.run_in_executor(pool,
                    asyncio.gather,
                    calc__particle_total_volume(),
                    calc__N_haloes_top_level(),
                    calc__N_halo_particles()
                )
                while not future.done(): pass
                future.result()

            data.N_halo_particles = sum([len(catalogue.get_particle_IDs(p)) for p in ParticleType.get_all()])
            data.halo_particle_total_volume = sum([(ArrayReorder.create(snapshot.get_IDs(p), catalogue.get_particle_IDs(p))(snapshot.get_smoothing_lengths(p).to("Mpc"))**3).sum() for p in ParticleType.get_all()], start = unyt_quantity(0.0, units = "Mpc**3")) * (np.pi * 4/3)

        return data

    @staticmethod
    def initialise_partial_alt(snapshot: SnapshotBase, catalogue: CatalogueBase, minimal: bool = False) -> "SnapshotStatsDataset":
        data =  SnapshotStatsDataset()

        data.snapshot_filepath = snapshot.filepath
        data.catalogue_membership_filepath = catalogue.membership_filepath
        data.catalogue_properties_filepath = catalogue.properties_filepath
        data.redshift = catalogue.z
        data.N_particles = sum([snapshot.number_of_particles(p) for p in ParticleType.get_all()])
        for part_type in ParticleType.get_all():
            setattr(data, f"N_particles_{part_type.name.replace(' ', '_')}", snapshot.number_of_particles(part_type))
        data.N_haloes = len(catalogue)
        if data.N_haloes > 0:
            data.N_halo_children = catalogue.number_of_children
            data.N_halo_decendants = catalogue.number_of_decendants

        if not minimal:

            # Async operation functions

            async def calc__particle_total_volume():
                data.particle_total_volume = sum(
                    [
                        (smoothing_lengths.to("Mpc")**3).sum()
                        for smoothing_lengths
                        in await asyncio.gather(*[snapshot.get_smoothing_lengths_async(p) for p in ParticleType.get_all()])
                    ],
                    start = unyt_quantity(0.0, units = "Mpc**3")
                ) * (np.pi * 4/3)

            async def calc__N_haloes_top_level():
                data.N_haloes_top_level = int((await catalogue.get_halo_parent_IDs_async() == -1).sum())

            async def calc__N_halo_particles_of_type(part_type: ParticleType):
                return len(await catalogue.get_particle_IDs_async(part_type))
            async def calc__N_halo_particles():
                (
                    data.N_halo_particles_gas,
                    data.N_halo_particles_star,
                    data.N_halo_particles_black_hole,
                    data.N_halo_particles_dark_matter,
                ) = await asyncio.gather(
                    calc__N_halo_particles_of_type(ParticleType.gas),
                    calc__N_halo_particles_of_type(ParticleType.star),
                    calc__N_halo_particles_of_type(ParticleType.black_hole),
                    calc__N_halo_particles_of_type(ParticleType.dark_matter)
                )

            # Run all async functions
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                future = loop.run_in_executor(pool,
                    asyncio.gather,
                    calc__particle_total_volume(),
                    calc__N_haloes_top_level(),
                    calc__N_halo_particles()
                )
                while not future.done(): pass
                future.result()

            data.N_halo_particles = sum([len(catalogue.get_particle_IDs(p)) for p in ParticleType.get_all()])
            data.halo_particle_total_volume = sum([(ArrayReorder.create(snapshot.get_IDs(p), catalogue.get_particle_IDs(p))(snapshot.get_smoothing_lengths(p).to("Mpc"))**3).sum() for p in ParticleType.get_all()], start = unyt_quantity(0.0, units = "Mpc**3")) * (np.pi * 4/3)

        return data



class ContraData(Struct):
    """
    Data output by Contra.

    To read a file, call load() and pass the filepath.
    """

    def __init__(self, *args, **kwargs) -> None:
        self.__target_snapshot: Union[SnapshotBase, None] = None
        self.__target_catalogue: Union[CatalogueBase, None] = None
        super().__init__(*args, **kwargs)

    header: HeaderDataset = TypedAutoProperty[HeaderDataset](TypeShield(HeaderDataset),
                doc = "Header. Data about the Contra run and output file.")
    data: Dict[ParticleType, ParticleTypeDataset] = TypedAutoProperty[Dict[ParticleType, ParticleTypeDataset]](TypeShield(dict),#TODO: dict nested shield? (not cast!!!)
                doc = "Datasets by particle type.")
    snapshot_search_stats: Union[List[SnapshotStatsDataset], None] = NullableTypedAutoProperty[List[SnapshotStatsDataset]](NestedTypeShield(list, SnapshotStatsDataset),
                doc = "Search stats for each searched snapshot.")

    @property
    def gas(self) -> Union[ParticleTypeDataset, None]:
        return self.data[ParticleType.gas] if ParticleType.gas in self.data else None
    @property
    def stars(self) -> Union[ParticleTypeDataset, None]:
        return self.data[ParticleType.star] if ParticleType.star in self.data else None
    @property
    def black_holes(self) -> Union[ParticleTypeDataset, None]:
        return self.data[ParticleType.black_hole] if ParticleType.black_hole in self.data else None
    @property
    def dark_matter(self) -> Union[ParticleTypeDataset, None]:
        return self.data[ParticleType.dark_matter] if ParticleType.dark_matter in self.data else None
    
    def _get_snapshot(self, filepath: str) -> SnapshotBase:
        if self.header.simulation_type == "SWIFT":
            return SnapshotSWIFT(filepath)
        elif self.header.simulation_type == "EAGLE":
            return SnapshotEAGLE(filepath)
        else:
            raise NotImplementedError(f"\"ContraData._get_snapshot\" not implemented for source simulation type \"{self.header.simulation_type}\".")
    
    def get_target_snapshot(self) -> SnapshotBase:
        if self.__target_snapshot is None:
            self.__target_snapshot =  self._get_snapshot(self.header.target_snapshot)
        return self.__target_snapshot

    def get_snapshot(self, stats: SnapshotStatsDataset) -> SnapshotBase:
        return self._get_snapshot(stats.snapshot_filepath)
    
    def _get_catalogue(self, membership_filepath: str, properties_filepath: str, snapshot: SnapshotBase) -> CatalogueBase:
        if self.header.simulation_type == "SWIFT":
            return CatalogueSOAP(membership_filepath, properties_filepath, snapshot)
        elif self.header.simulation_type == "EAGLE":
            return CatalogueSUBFIND(membership_filepath, properties_filepath, snapshot)
        else:
            raise NotImplementedError(f"\"ContraData._get_catalogue\" not implemented for source simulation type \"{self.header.simulation_type}\".")
    
    def get_target_catalogue(self) -> CatalogueBase:
        if self.__target_catalogue is None:
            self.__target_catalogue = self._get_catalogue(self.header.target_catalogue_membership_file, self.header.target_catalogue_properties_file, self.get_target_snapshot())
        return self.__target_catalogue

    def get_catalogue(self, stats: SnapshotStatsDataset) -> CatalogueBase:
        return self._get_catalogue(stats.catalogue_membership_filepath, stats.catalogue_properties_filepath, self.get_snapshot(stats))

    @singledispatchmethod
    def get_matched_particles_mask(self, redshift: Any, part_type: ParticleType):
        """
        Creates a numpy mask array for the target snapshot data (for the requested particle type)
        that selects particles last found in a halo at the specified redshift.
        """
        try:
            return self.get_matched_particles_mask(float(redshift), part_type)
        except:
            raise TypeError(f"Invalid argument type {type(redshift)} for first parameter of ContraData.get_matched_particles_mask")
    @get_matched_particles_mask.register(float)
    def _(self, redshift: float, part_type: ParticleType):
        if part_type not in self.data:
            raise KeyError(f"No search data avalible for {part_type.name} particles.")
        return self.data[part_type].redshifts == redshift
    @get_matched_particles_mask.register(SnapshotStatsDataset)
    def _(self, stats: SnapshotStatsDataset, part_type: ParticleType):
        return self.get_matched_particles_mask(stats.redshift, part_type)
    @get_matched_particles_mask.register(SnapshotBase)
    def _(self, snapshot: SnapshotBase, part_type: ParticleType):
        return self.get_matched_particles_mask(snapshot.redshift, part_type)
    @get_matched_particles_mask.register(CatalogueBase)
    def _(self, catalogue: CatalogueBase, part_type: ParticleType):
        return self.get_matched_particles_mask(catalogue.redshift, part_type)

    @singledispatchmethod
    def get_snapshot_indexes_of_matched_particles(self, *args, **kwargs):
        """
        Get a numpy array of indexes to the specified snapshot's specified particle type data that
        returns particle data for the particles last found in haloes at that snapshot's redshift.

        Call signitures:

                                             part_type (ParticleType)

            stats (SnapshotStatsDataset),    part_type (ParticleType)

            snapshot (SnapshotBase),         part_type (ParticleType)

            catalogue (CatalogueBase),       part_type (ParticleType)
        """
        if len(args) > 0:
            raise TypeError(f"Invalid argument type {type(args[0])} for first parameter of ContraData.get_snapshot_indexes_of_matched_particles")
        else:
            RuntimeError("This should be impossible! Please report!")
    @get_snapshot_indexes_of_matched_particles.register(ParticleType)
    def _(self, part_type: ParticleType):
        if part_type not in self.data:
            raise KeyError(f"No search data avalible for {part_type.name} particles.")
        target_snap_mask = self.get_matched_particles_mask(self.get_target_snapshot().redshift, part_type)
        return np.where(target_snap_mask)[0]
    @get_snapshot_indexes_of_matched_particles.register(SnapshotStatsDataset)
    def _(self, stats: SnapshotStatsDataset, part_type: ParticleType):
        return self.get_snapshot_indexes_of_matched_particles(self.get_snapshot(stats), part_type)
    @get_snapshot_indexes_of_matched_particles.register(SnapshotBase)
    def _(self, snapshot: SnapshotBase, part_type: ParticleType):
        if part_type not in self.data:
            raise KeyError(f"No search data avalible for {part_type.name} particles.")
        target_snap_mask = self.get_matched_particles_mask(snapshot.redshift, part_type)
        target_snap_ids = self.get_target_snapshot().get_IDs(part_type)[target_snap_mask]
        sorted_target_snap_ids_indexes = np.argsort(target_snap_ids)
        return np.searchsorted(target_snap_ids, snapshot.get_IDs(part_type), sorter = sorted_target_snap_ids_indexes)[np.argsort(sorted_target_snap_ids_indexes)]
    @get_snapshot_indexes_of_matched_particles.register(CatalogueBase)
    def _(self, catalogue: CatalogueBase, part_type: ParticleType):
        return self.get_snapshot_indexes_of_matched_particles(catalogue.snapshot, part_type)
    
    @staticmethod
    def load(filepath: str) -> "ContraData":
        result = None
        reader = OutputReader(filepath)
        with reader:
            result = reader.read()
        return result



class OutputWriter(object):
    """
    Writes datasets to an HDF5 file.
    """

    def __init__(self, filepath: str, overwrite = False) -> None:
        self.__filepath = filepath
        self.__file = None

        # Create the file
        f = h5.File(self.__filepath, "w" if overwrite else "a")
        if overwrite or "SnapshotStats" not in f:
            f.create_group("SnapshotStats")
        f.close()

    @property
    def is_open(self):
        return self.__file is not None
    
    def open(self):
        self.__file = h5.File(self.__filepath, "a")

    def close(self):
        self.__file.close()
        self.__file = None

    def __enter__(self) -> "OutputWriter":
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        self.close()

    def write_header(self, header: HeaderDataset) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        d = self.__file.create_group("Header")
        d.attrs["Version"] = str(header.version)
        d.attrs["Date"] = datetime.datetime(year = header.date.year, month = header.date.month, day = header.date.day).timestamp()
        d.attrs["SnapshotFilepath"] = header.target_snapshot
        d.attrs["CatalogueMembershipFilepath"] = header.target_catalogue_membership_file
        d.attrs["CataloguePropertiesFilepath"] = header.target_catalogue_properties_file
        d.attrs["SimulationType"] = header.simulation_type
        d.attrs["Redshift"] = header.redshift
        d.attrs["NumberSearchedSnapshots"] = header.N_searched_snapshots
#        d.attrs["SearchedFiles"] = header.searched_files
        d.attrs["OutputFile"] = header.output_file
        d.attrs["HasGas"] = header.has_gas
        d.attrs["HasStars"] = header.has_stars
        d.attrs["HasBlackHoles"] = header.has_black_holes
        d.attrs["HasDarkMatter"] = header.has_dark_matter
        d.attrs["HasStatistics"] = header.has_statistics

    def increase_number_of_snapshots(self, number_of_snapshots: int) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if "Header" not in self.__file:
            raise KeyError("No header dataset to update! Create a header first.")
        self.__file["Header"].attrs["NumberSearchedSnapshots"] = number_of_snapshots

    def write_checkpoint(self, dataset: CheckpointData) -> None:

        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")

        old_dataset_name = f"{dataset.particle_type.common_hdf5_name}_old"
        new_dataset_name = dataset.particle_type.common_hdf5_name

        # Create the "Checkpoint" group if it dosen't already exist
        if "Checkpoint" not in self.__file:
            c = self.__file.create_group("Checkpoint")
        else:
            c = self.__file["Checkpoint"]
            # Delete old checkpoint backup if present
            if old_dataset_name in c:
                del c[old_dataset_name]
            # Backup last checkpoint if present
            if new_dataset_name in c:
                c.move(new_dataset_name, old_dataset_name)

        # Create new checkpoint
        d = c.create_group(new_dataset_name)
        d.create_dataset(name = "HaloRedshift", data = np.array(dataset.redshifts, dtype = np.float32))
        d.create_dataset(name = "HaloID", data = np.array(dataset.halo_ids, dtype = np.int32))
        d.create_dataset(name = "HaloMass", data = np.array(dataset.halo_masses, dtype = np.float32))
        d.create_dataset(name = "RelitiveHaloMass", data = np.array(dataset.halo_masses_scaled, dtype = np.float32))
        d.create_dataset(name = "PositionPreEjection", data = np.array(dataset.positions_pre_ejection, dtype = np.float32))
        d.create_dataset(name = "MissingParticlesMask", data = np.array(dataset.missing_particles_mask, dtype = np.bool_))
        d.attrs["SnapshotSearchIndex"] = dataset.last_complete_snap_index


    def write_particle_type_dataset(self, dataset: ParticleTypeDataset, overwrite: bool = False) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if dataset.particle_type.common_hdf5_name in self.__file:
            if overwrite:
                del self.__file[dataset.particle_type.common_hdf5_name]
            else:
                raise KeyError("Output dataset already exists. Specify \"overwrite\" to allow overwriting of existing datasets.")
        d = self.__file.create_group(dataset.particle_type.common_hdf5_name)
        d.create_dataset(name = "HaloRedshift", data = dataset.redshifts)
        d.create_dataset(name = "HaloID", data = dataset.halo_ids)
        d.create_dataset(name = "HaloMass", data = dataset.halo_masses)
        d.create_dataset(name = "RelitiveHaloMass", data = dataset.halo_masses_scaled)
        d.create_dataset(name = "PositionPreEjection", data = dataset.positions_pre_ejection)

    def write_snapshot_stats_dataset(self, index: int, stats: SnapshotStatsDataset, minimal = False) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        d = self.__file["SnapshotStats"].create_group(str(index))
        d.attrs["SnapshotFilepath"] = stats.snapshot_filepath
        d.attrs["CatalogueMembershipFilepath"] = stats.catalogue_membership_filepath
        d.attrs["CataloguePropertiesFilepath"] = stats.catalogue_properties_filepath
        d.attrs["Redshift"] = stats.redshift
        d.attrs["NumberOfParticles"] = stats.N_particles
        d.attrs["NumberOfHaloes"] = stats.N_haloes
        if not minimal:
            d.attrs["NumberOfTopLevelHaloes"] = stats.N_haloes_top_level
            d.attrs["NumberOfHaloParticles"] = stats.N_halo_particles
        d.attrs["NumberOfMatchedParticles"] = stats.N_particles_matched
        if not minimal:
            d.attrs["VolumeOfParticles"] = stats.particle_total_volume.to("Mpc**3").value
            d.attrs["VolumeOfHaloParticles"] = stats.halo_particle_total_volume.to("Mpc**3").value
            d.attrs["VolumeOfMatchedParticles"] = stats.particles_matched_total_volume.to("Mpc**3").value

        d.create_dataset(name = "NumberOfTypedParticles", data = np.array([
            (stats.N_particles_gas         if stats.N_particles_gas         is not None else -1),
            (stats.N_particles_star        if stats.N_particles_star        is not None else -1),
            (stats.N_particles_black_hole  if stats.N_particles_black_hole  is not None else -1),
            (stats.N_particles_dark_matter if stats.N_particles_dark_matter is not None else -1)
        ], dtype = int))
        d.create_dataset(name = "NumberOfHaloTypedParticles", data = np.array([
            (stats.N_halo_particles_gas         if stats.N_halo_particles_gas         is not None else -1),
            (stats.N_halo_particles_star        if stats.N_halo_particles_star        is not None else -1),
            (stats.N_halo_particles_black_hole  if stats.N_halo_particles_black_hole  is not None else -1),
            (stats.N_halo_particles_dark_matter if stats.N_halo_particles_dark_matter is not None else -1)
        ], dtype = int))
        d.create_dataset(name = "NumberOfMatchedTypedParticles", data = np.array([
            (stats.N_particles_matched_gas         if stats.N_particles_matched_gas         is not None else -1),
            (stats.N_particles_matched_star        if stats.N_particles_matched_star        is not None else -1),
            (stats.N_particles_matched_black_hole  if stats.N_particles_matched_black_hole  is not None else -1),
            (stats.N_particles_matched_dark_matter if stats.N_particles_matched_dark_matter is not None else -1)
        ], dtype = int))
        d.create_dataset(name = "VolumeOfMatchedTypedParticles", data = np.array([
            (stats.particles_matched_total_volume_gas.to("Mpc**3").value         if stats.particles_matched_total_volume_gas         is not None else -1),
            (stats.particles_matched_total_volume_star.to("Mpc**3").value        if stats.particles_matched_total_volume_star        is not None else -1),
            (stats.particles_matched_total_volume_black_hole.to("Mpc**3").value  if stats.particles_matched_total_volume_black_hole  is not None else -1),
            (stats.particles_matched_total_volume_dark_matter.to("Mpc**3").value if stats.particles_matched_total_volume_dark_matter is not None else -1)
        ], dtype = float))
        if stats.N_haloes > 0:
            d.create_dataset(name = "HaloesNumberOfChildren", data = stats.N_halo_children)
            d.create_dataset(name = "HaloesNumberOfDecendants", data = stats.N_halo_decendants)

    def write(self, data: ContraData) -> None:
        force_open = not self.is_open
        if force_open:
            self.open()
        self.write_header(data.header)
        for key in data.data:
            self.write_particle_type_dataset(data.data[key])
        if data.header.has_statistics:
            for i, stats in enumerate(data.snapshot_search_stats):
                self.write_snapshot_stats_dataset(i, stats)
        if force_open:
            self.close()



class OutputReader(object):
    """
    Reads saved Contra data.
    """

    def __init__(self, filepath: str) -> None:
        self.__filepath = filepath
        self.__file = None

    @property
    def is_open(self):
        return self.__file is not None
    
    def open(self):
        self.__file = h5.File(self.__filepath, "r")

    def close(self):
        self.__file.close()
        self.__file = None

    def __enter__(self) -> "OutputReader":
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        self.close()

    def read_header(self) -> HeaderDataset:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        s = HeaderDataset()
        s.version = VersionInfomation.from_string(self.__file["Header"].attrs["Version"])
        s.date = datetime.datetime.fromtimestamp((self.__file["Header"].attrs["Date"])).date()
        s.target_snapshot = str(self.__file["Header"].attrs["SnapshotFilepath"])
        s.target_catalogue_membership_file = str(self.__file["Header"].attrs["CatalogueMembershipFilepath"])
        s.target_catalogue_properties_file = str(self.__file["Header"].attrs["CataloguePropertiesFilepath"])
        s.simulation_type = str(self.__file["Header"].attrs["SimulationType"])
        s.redshift = float(self.__file["Header"].attrs["Redshift"])
        s.N_searched_snapshots = int(self.__file["Header"].attrs["NumberSearchedSnapshots"])
#        s.searched_files = self.__file["Header"].attrs["SearchedFiles"]
        s.output_file = str(self.__file["Header"].attrs["OutputFile"])
        s.has_gas = bool(self.__file["Header"].attrs["HasGas"])
        s.has_stars = bool(self.__file["Header"].attrs["HasStars"])
        s.has_black_holes = bool(self.__file["Header"].attrs["HasBlackHoles"])
        s.has_dark_matter = bool(self.__file["Header"].attrs["HasDarkMatter"])
        s.has_statistics = bool(self.__file["Header"].attrs["HasStatistics"])
        return s
    
    def read_checkpoint(self, part_type: ParticleType) -> CheckpointData:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if "Checkpoint" not in self.__file:
            raise IOError("No checkpoints avalible.")
        elif part_type.common_hdf5_name not in self.__file["Checkpoint"]:
            raise KeyError(f"No checkpoint data for {part_type.name} particles.")
        d = self.__file[f"Checkpoint/{part_type.common_hdf5_name}"]
        return CheckpointData(
            redshifts = np.array(d["HaloRedshift"][:], dtype = np.float64),
            halo_ids = np.array(d["HaloID"][:], dtype = np.int64),
            halo_masses = np.array(d["HaloMass"][:], dtype = np.float64),
            halo_masses_scaled = np.array(d["RelitiveHaloMass"][:], dtype = np.float64),
            positions_pre_ejection = np.array(d["PositionPreEjection"][:], dtype = np.float64),
            missing_particles_mask = np.array(d["MissingParticlesMask"][:], dtype = np.bool_),
            last_complete_snap_index = int(d.attrs["SnapshotSearchIndex"])
        )

    def read_particle_type_dataset(self, part_type: ParticleType) -> ParticleTypeDataset:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if part_type.common_hdf5_name not in self.__file:
            raise KeyError(f"Contra output file has no dataset \"{part_type.common_hdf5_name}\".")
        d = self.__file[part_type.common_hdf5_name]
        s = ParticleTypeDataset()
        s.redshifts = d["HaloRedshift"][:]
        s.halo_ids = d["HaloID"][:]
        s.halo_masses = d["HaloMass"][:]
        s.halo_masses_scaled = d["RelitiveHaloMass"][:]
        s.positions_pre_ejection = d["PositionPreEjection"][:]
        return s

    def read_snapshot_stats_dataset(self, index: int) -> SnapshotStatsDataset:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        n_snapshots = int(self.__file["Header"].attrs["NumberSearchedSnapshots"])
        true_index = index if index >= 0 else (n_snapshots - index)
        if true_index >= n_snapshots:
            raise IndexError(f"Index {index} out of bounds for number of snapshots avalible ({n_snapshots}).")
        d = self.__file[f"SnapshotStats/{index}"]
        s = SnapshotStatsDataset()

        s.snapshot_filepath = str(d.attrs["SnapshotFilepath"])
        s.catalogue_membership_filepath = str(d.attrs["CatalogueMembershipFilepath"])
        s.catalogue_properties_filepath = str(d.attrs["CataloguePropertiesFilepath"])
        s.redshift = float(d.attrs["Redshift"])
        s.N_particles = int(d.attrs["NumberOfParticles"])
        s.N_haloes = int(d.attrs["NumberOfHaloes"])
        if "NumberOfTopLevelHaloes" in d.attrs:
            s.N_haloes_top_level = int(d.attrs["NumberOfTopLevelHaloes"])
        if "NumberOfHaloParticles" in d.attrs:
            s.N_halo_particles = int(d.attrs["NumberOfHaloParticles"])
        if "NumberOfMatchedParticles" in d.attrs:
            s.N_particles_matched = int(d.attrs["NumberOfMatchedParticles"])
        if "VolumeOfParticles" in d.attrs:
            s.particle_total_volume = unyt_quantity(d.attrs["VolumeOfParticles"], units = "Mpc**3")
        if "VolumeOfHaloParticles" in d.attrs:
            s.halo_particle_total_volume = unyt_quantity(d.attrs["VolumeOfHaloParticles"], units = "Mpc**3")
        if "VolumeOfMatchedParticles" in d.attrs:
            s.particles_matched_total_volume = unyt_quantity(d.attrs["VolumeOfMatchedParticles"], units = "Mpc**3")

        N_particles__by_type = d["NumberOfTypedParticles"][:]
        if N_particles__by_type[0] > -1:
            s.N_particles_gas = int(N_particles__by_type[0])
        if N_particles__by_type[1] > -1:
            s.N_particles_star = int(N_particles__by_type[1])
        if N_particles__by_type[2] > -1:
            s.N_particles_black_hole = int(N_particles__by_type[2])
        if N_particles__by_type[3] > -1:
            s.N_particles_dark_matter = int(N_particles__by_type[3])

        N_halo_particles__by_type = d["NumberOfHaloTypedParticles"][:]
        if N_halo_particles__by_type[0] > -1:
            s.N_halo_particles_gas = int(N_halo_particles__by_type[0])
        if N_halo_particles__by_type[1] > -1:
            s.N_halo_particles_star = int(N_halo_particles__by_type[1])
        if N_halo_particles__by_type[2] > -1:
            s.N_halo_particles_black_hole = int(N_halo_particles__by_type[2])
        if N_halo_particles__by_type[3] > -1:
            s.N_halo_particles_dark_matter = int(N_halo_particles__by_type[3])

        N_particles_matched__by_type = d["NumberOfMatchedTypedParticles"][:]
        if N_particles_matched__by_type[0] > -1:
            s.N_particles_matched_gas = int(N_particles_matched__by_type[0])
        if N_particles_matched__by_type[1] > -1:
            s.N_particles_matched_star = int(N_particles_matched__by_type[1])
        if N_particles_matched__by_type[2] > -1:
            s.N_particles_matched_black_hole = int(N_particles_matched__by_type[2])
        if N_particles_matched__by_type[3] > -1:
            s.N_particles_matched_dark_matter = int(N_particles_matched__by_type[3])

        particles_matched_total_volume__by_type = d["VolumeOfMatchedTypedParticles"][:]
        if particles_matched_total_volume__by_type[0] > -1:
            s.particles_matched_total_volume_gas = unyt_quantity(particles_matched_total_volume__by_type[0], units = "Mpc**3")
        if particles_matched_total_volume__by_type[1] > -1:
            s.particles_matched_total_volume_star = unyt_quantity(particles_matched_total_volume__by_type[1], units = "Mpc**3")
        if particles_matched_total_volume__by_type[2] > -1:
            s.particles_matched_total_volume_black_hole = unyt_quantity(particles_matched_total_volume__by_type[2], units = "Mpc**3")
        if particles_matched_total_volume__by_type[3] > -1:
            s.particles_matched_total_volume_dark_matter = unyt_quantity(particles_matched_total_volume__by_type[3], units = "Mpc**3")

        if s.N_haloes > 0:
            s.N_halo_children = d["HaloesNumberOfChildren"][:]
            s.N_halo_decendants = d["HaloesNumberOfDecendants"][:]

        return s
    
    def read(self) -> ContraData:
        force_open = not self.is_open
        if force_open:
            self.open()
        s = ContraData()
        s.header = self.read_header()
        loaded_data = {}
        for p in ParticleType.get_all():
            try:
                loaded_data[p] = self.read_particle_type_dataset(p)
            except:
                loaded_data.pop(p, None)
        s.data = loaded_data
        if s.header.has_statistics:
            loaded_snap_stats = []
            for i in range(s.header.N_searched_snapshots):
                loaded_snap_stats.append(self.read_snapshot_stats_dataset(i))
            s.snapshot_search_stats = loaded_snap_stats
        else:
            s.snapshot_search_stats = None
        if force_open:
            self.close()
        return s
