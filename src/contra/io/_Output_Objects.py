# SPDX-FileCopyrightText: 2024-present Christopher Rowe <chris.rowe19@outlook.com>
#
# SPDX-License-Identifier: None
import datetime
from typing import cast, Any, Union, Collection, List, Dict
import asyncio
from concurrent.futures import ThreadPoolExecutor
from functools import singledispatchmethod
import os

import numpy as np
import h5py as h5
from unyt import unyt_quantity
from QuasarCode import Console, Settings
from QuasarCode.Data import VersionInfomation
from QuasarCode.Tools import Struct, TypedAutoProperty, NullableTypedAutoProperty, TypeShield, NestedTypeShield
from QuasarCode.MPI import MPI_Config, mpi_barrier, mpi_get_slice

from .._ArrayReorder import ArrayReorder
from .._ParticleType import ParticleType
from ._EAGLE import FileTreeScraper_EAGLE
#from ._SWIFT import FileTreeScraper_SWIFT#TODO:
from ._SnapshotBase import SnapshotBase
from ._SnapshotSWIFT import SnapshotSWIFT
from ._SnapshotEAGLE import SnapshotEAGLE
from ._CatalogueBase import CatalogueBase
from ._CatalogueSOAP import CatalogueSOAP
from ._CatalogueSUBFIND import CatalogueSUBFIND

#os.environ["HDF5_USE_FILE_LOCKING"] = "FALSE"



class HeaderDataset(Struct):
    """
    version

    date

    simulation_type

    simulation_directory

    N_searched_snapshots

    last_file_number

    uses_snipshots

    output_file

    has_gas

    has_stars

    has_black_holes

    has_dark_matter

    has_statistics
    """

    version = TypedAutoProperty[VersionInfomation](TypeShield(VersionInfomation),
                doc = "Contra version number.")
    date = TypedAutoProperty[datetime.date](TypeShield(datetime.date),
                doc = "Date of execution (start of program).")
    simulation_type = TypedAutoProperty[str](TypeShield(str),
                doc = "Type of source simulation data.")
    simulation_directory = TypedAutoProperty[str](TypeShield(str),
                doc = "Path to the directory containing simulation data.")
    N_searched_snapshots = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of snapshots searched.")
    last_file_number = TypedAutoProperty[str](TypeShield(str),
                doc = "Number string of the last file searched (lowest redshift).")
    uses_snipshots = TypedAutoProperty[bool](TypeShield(bool),
                doc = "Search uses snipshot files instead of snapshot files.")
    output_file = TypedAutoProperty[str](TypeShield(str),
                doc = "This output file location as generated by Contra.")
    has_gas = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for gas particles.")
    has_stars = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for star particles.")
    has_black_holes = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for black hole particles.")
    has_dark_matter = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains a dataset for dark matter particles.")
    has_statistics = TypedAutoProperty[bool](TypeShield(bool),
                doc = "This output file contains statistics for each snapshot searched.")

    @staticmethod
    def _read(file: h5.File, parallel: bool) -> "HeaderDataset":
        if parallel:
            mpi_barrier()
        h = file["Header"]
        data = HeaderDataset()
        data.version              =   VersionInfomation.from_string(h.attrs["Version"]                )
        data.date                 = datetime.datetime.fromtimestamp(h.attrs["Date"]                   ).date()
        data.simulation_type      =                             str(h.attrs["SimulationType"]         )
        data.simulation_directory =                             str(h.attrs["SimulationDirectory"]    )
        data.N_searched_snapshots =                             int(h.attrs["NumberSearchedSnapshots"])
        data.uses_snipshots       =                            bool(h.attrs["UsesSnipshots"]          )
        data.output_file          =                             str(h.attrs["OutputFile"]             )
        data.has_gas              =                            bool(h.attrs["HasGas"]                 )
        data.has_stars            =                            bool(h.attrs["HasStars"]               )
        data.has_black_holes      =                            bool(h.attrs["HasBlackHoles"]          )
        data.has_dark_matter      =                            bool(h.attrs["HasDarkMatter"]          )
        data.has_statistics       =                            bool(h.attrs["HasStatistics"]          )
        if parallel:
            mpi_barrier()
        return data

    @staticmethod
    def _write(file: h5.File, data: "HeaderDataset", parallel: bool, root: int|None = None) -> h5.Group:
        if parallel:
            mpi_barrier()
        h: h5.Group = file["Header"] if "Header" in file else file.create_group("Header")
        if parallel:
            mpi_barrier()
        #if MPI_Config.check_is_root(root):
        #    h.attrs["Version"]                 = str(data.version)
        #    h.attrs["Date"]                    = datetime.datetime(year = data.date.year, month = data.date.month, day = data.date.day).timestamp()
        #    h.attrs["SimulationType"]          = data.simulation_type
        #    h.attrs["SimulationDirectory"]     = data.simulation_directory
        #    h.attrs["NumberSearchedSnapshots"] = data.N_searched_snapshots
        #    h.attrs["UsesSnipshots"]           = data.uses_snipshots
        #    h.attrs["OutputFile"]              = data.output_file
        #    h.attrs["HasGas"]                  = data.has_gas
        #    h.attrs["HasStars"]                = data.has_stars
        #    h.attrs["HasBlackHoles"]           = data.has_black_holes
        #    h.attrs["HasDarkMatter"]           = data.has_dark_matter
        #    h.attrs["HasStatistics"]           = data.has_statistics
        h.attrs["Version"]                 = str(data.version)
        h.attrs["Date"]                    = datetime.datetime(year = data.date.year, month = data.date.month, day = data.date.day).timestamp()
        h.attrs["SimulationType"]          = data.simulation_type
        h.attrs["SimulationDirectory"]     = data.simulation_directory
        h.attrs["NumberSearchedSnapshots"] = data.N_searched_snapshots
        h.attrs["UsesSnipshots"]           = data.uses_snipshots
        h.attrs["OutputFile"]              = data.output_file
        h.attrs["HasGas"]                  = data.has_gas
        h.attrs["HasStars"]                = data.has_stars
        h.attrs["HasBlackHoles"]           = data.has_black_holes
        h.attrs["HasDarkMatter"]           = data.has_dark_matter
        h.attrs["HasStatistics"]           = data.has_statistics

        if parallel:
            mpi_barrier()
        return h



class ParticleTypeDataset(Struct):#TODO: data is only float/int 32 bit precision!
    """
    particle_type

    target_redshift

    file_number

    length

    length_this_rank

    redshifts

    halo_ids

    halo_masses

    halo_masses_scaled

    locations_pre_ejection
    """

    particle_type = TypedAutoProperty[ParticleType](TypeShield(ParticleType),
                doc = "Particle type.")
    target_redshift = TypedAutoProperty[float](TypeShield(float),
                doc = "Redshift of the particle distribution.")
    file_number = TypedAutoProperty[str](TypeShield(str),
                doc = "File number string of the particle distribution.")
    length = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of particles in each field (irrespective of MPI rank partitioning).")
    length_this_rank = TypedAutoProperty[int](TypeShield(int),
                doc = "Actual number of elements in each field.")
    redshifts = TypedAutoProperty[Collection[float]](NestedTypeShield(np.ndarray, np.float64),
                doc = "Redshift of last halo membership.")
    halo_ids = TypedAutoProperty[Collection[int]](NestedTypeShield(np.ndarray, np.int64),
                doc = "ID of last halo.")
    halo_masses = TypedAutoProperty[Collection[float]](NestedTypeShield(np.ndarray, np.float64),#TODO: make unyt
                doc = "Mass of last halo.")
    halo_masses_scaled = TypedAutoProperty[Collection[float]](NestedTypeShield(np.ndarray, np.float64),
                doc = "Mass of last halo as a fraction of the L_* mass at that redshift.")
    positions_pre_ejection = TypedAutoProperty[Collection[Collection[float]]](NestedTypeShield(np.ndarray, np.ndarray, np.float64),#TODO: make unyt
                doc = "Particle coordinates prior to final ejection.")

    def get_matched_particles_mask(self, min_redshift: float|None = None) -> np.ndarray:
        """
        Create a mask array that selects particles with a last known halo.
        Potionally, specify a redshift to mask only particles that were last in a halo at or above the redshift.
        """
        if min_redshift is None:
            return self.halo_ids != -1
        else:
            return (self.redshifts >= min_redshift)

    @staticmethod
    def _read(file: h5.File, part_type: ParticleType, number: str, parallel: bool) -> "ParticleTypeDataset":
        if part_type.common_hdf5_name not in file:
            raise KeyError(f"Contra output file has no dataset \"{part_type.common_hdf5_name}\".")
        if number not in file[part_type.common_hdf5_name]:
            raise KeyError(f"Contra output file has no {part_type.name} particle dataset for file number {number}.")

        if parallel:
            mpi_barrier()

        d = file[part_type.common_hdf5_name][number]

        # If MPI is not being used, this is equivilant to the whole dataset regardless of parallel being set
        data_slice = mpi_get_slice(d["HaloID"].shape[0]) if parallel else slice(d["HaloID"].shape[0])

        if parallel:
            mpi_barrier()

        data = ParticleTypeDataset()
        data.particle_type          = part_type
        data.target_redshift        = d.attrs["Redshift"]
        data.file_number            = number
        data.redshifts              = d["HaloRedshift"][data_slice]
        data.halo_ids               = d["HaloID"][data_slice]
        data.halo_masses            = d["HaloMass"][data_slice]
        data.halo_masses_scaled     = d["RelitiveHaloMass"][data_slice]
        data.positions_pre_ejection = d["PositionPreEjection"][data_slice]
        data.length                 = d["HaloID"].shape[0]
        data.length_this_rank       = data.halo_ids.shape[0]

        if parallel:
            mpi_barrier()

        return data

    @staticmethod
    def _write(file: h5.File, data: "ParticleTypeDataset", parallel: bool) -> h5.Group:
        if data.particle_type.common_hdf5_name not in file:
            if parallel:
                mpi_barrier()
            file.create_group(data.particle_type.common_hdf5_name)

        if parallel:
            mpi_barrier()

        d = file[data.particle_type.common_hdf5_name][data.file_number] if data.file_number in file[data.particle_type.common_hdf5_name] else file[data.particle_type.common_hdf5_name].create_group(data.file_number)

        if parallel:
            mpi_barrier()

        #if MPI_Config.is_root:
        #    d.attrs["Redshift"] = data.target_redshift
        d.attrs["Redshift"] = data.target_redshift

        if parallel:
            mpi_barrier()
            group_length = data.length_this_rank#len(data)
#            end_offsets = np.cumsum(np.concatenate(MPI_Config.comm.allgather(group_length)))
            end_offsets = np.cumsum(MPI_Config.comm.allgather(group_length))
            group_length = end_offsets[-1]
            rank_slice = slice(end_offsets[MPI_Config.rank - 1] if MPI_Config.rank > 0 else 0, end_offsets[MPI_Config.rank])
            d.create_dataset(name = "HaloRedshift",        shape = (group_length,),   dtype = data.redshifts.dtype             )[rank_slice] = data.redshifts
            d.create_dataset(name = "HaloID",              shape = (group_length,),   dtype = data.halo_ids.dtype              )[rank_slice] = data.halo_ids
            d.create_dataset(name = "HaloMass",            shape = (group_length,),   dtype = data.halo_masses.dtype           )[rank_slice] = data.halo_masses
            d.create_dataset(name = "RelitiveHaloMass",    shape = (group_length,),   dtype = data.halo_masses_scaled.dtype    )[rank_slice] = data.halo_masses_scaled
            d.create_dataset(name = "PositionPreEjection", shape = (group_length, 3), dtype = data.positions_pre_ejection.dtype)[rank_slice] = data.positions_pre_ejection
            mpi_barrier()

#            print(MPI_Config.rank, (d["HaloID"][rank_slice] != data.halo_ids).sum(), flush = True)
#            collected_inputs = MPI_Config.comm.gather(data.halo_ids)
#            if MPI_Config.is_root:
#                collected_inputs = np.concatenate(collected_inputs)
#                Console.print_raw("Written data != input data:", (d["HaloID"][:] != collected_inputs).sum())

        else:
            d.create_dataset(name = "HaloRedshift",        data = data.redshifts)
            d.create_dataset(name = "HaloID",              data = data.halo_ids)
            d.create_dataset(name = "HaloMass",            data = data.halo_masses)
            d.create_dataset(name = "RelitiveHaloMass",    data = data.halo_masses_scaled)
            d.create_dataset(name = "PositionPreEjection", data = data.positions_pre_ejection)

        return d
    


'''
class _ParticleTypeDataset_AutoLoad(ParticleTypeDataset):
    def __init__(self, reader: "OutputReader", part_type: ParticleType, number: str, **kwargs) -> None:
        self.__reader = reader
        self.__part_type = part_type
        self.__number = number
        self.__loaded = False
        super().__init__(**kwargs)
    def ensure_loaded(self) -> None:
        if not self.__loaded:
            with self.__reader:
                data = self.__reader.read_particle_type_dataset(self.__part_type, self.__number)
            self.__loaded = True # Needed to avoid recursive calls to this method!
            self.particle_type = data.particle_type
            self.target_redshift = data.target_redshift
            self.file_number = data.file_number
            self.redshifts = data.redshifts
            self.halo_ids = data.halo_ids
            self.halo_masses = data.halo_masses
            self.halo_masses_scaled = data.halo_masses_scaled
            self.positions_pre_ejection = data.positions_pre_ejection

    @property
    def particle_type(self) -> ParticleType:
        self.ensure_loaded()
        return super().particle_type
    @particle_type.setter
    def particle_type(self, value: ParticleType) -> None:
        self.ensure_loaded()
        print(dir(super()))
        super().particle_type = value

    @property
    def target_redshift(self) -> ParticleType:
        self.ensure_loaded()
        return super().target_redshift
    @target_redshift.setter
    def target_redshift(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().target_redshift = value

    @property
    def file_number(self) -> ParticleType:
        self.ensure_loaded()
        return super().file_number
    @file_number.setter
    def file_number(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().file_number = value

    @property
    def redshifts(self) -> ParticleType:
        self.ensure_loaded()
        return super().redshifts
    @redshifts.setter
    def redshifts(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().redshifts = value

    @property
    def halo_ids(self) -> ParticleType:
        self.ensure_loaded()
        return super().halo_ids
    @halo_ids.setter
    def halo_ids(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().halo_ids = value

    @property
    def halo_masses(self) -> ParticleType:
        self.ensure_loaded()
        return super().halo_masses
    @halo_masses.setter
    def halo_masses(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().halo_masses = value

    @property
    def halo_masses_scaled(self) -> ParticleType:
        self.ensure_loaded()
        return super().halo_masses_scaled
    @halo_masses_scaled.setter
    def halo_masses_scaled(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().halo_masses_scaled = value

    @property
    def positions_pre_ejection(self) -> ParticleType:
        self.ensure_loaded()
        return super().positions_pre_ejection
    @positions_pre_ejection.setter
    def positions_pre_ejection(self, value: ParticleType) -> None:
        self.ensure_loaded()
        super().positions_pre_ejection = value
'''



'''
class CheckpointData(ParticleTypeDataset):
    """
    last_complete_snap_index

    missing_particles_mask
    """

    last_complete_snap_index = TypedAutoProperty[int](TypeShield(int),
                doc = "Index of last .")
    missing_particles_mask = TypedAutoProperty[np.ndarray](NestedTypeShield(np.ndarray, np.bool_),
                doc = "Boolean mask for particles that have not yet been found in haloes.")
'''
                


#TODO: add snapshot number
class SnapshotStatsDataset(Struct):
    """
    snapshot_filepath

    catalogue_membership_filepath

    catalogue_properties_filepath

    redshift

    N_particles

    N_particles_(gas | star | black_hole | dark_matter)

    N_haloes

    N_haloes_top_level

    N_halo_children

    N_halo_decendants

    N_halo_particles

    N_halo_particles_(gas | star | black_hole | dark_matter)

    N_particles_matched

    N_particles_matched_(gas | star | black_hole | dark_matter)

    particle_total_volume

    halo_particle_total_volume

    particles_matched_total_volume

    particles_matched_total_volume_(gas | star | black_hole | dark_matter)
    """

    snapshot_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of snapshot file.")
    catalogue_membership_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of catalogue file containing particle membership information.")
    catalogue_properties_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of catalogue file containing halo properties.")
    catalogue_filepath = TypedAutoProperty[str](TypeShield(str),
                doc = "Filepath of catalogue file.")
    redshift = TypedAutoProperty[float](TypeShield(float),
                doc = "Snapshot redshift.")
    N_particles = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of particles in the snapshot.")
    N_particles_gas = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of gas particles in the snapshot.")
    N_particles_star = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of star particles in the snapshot.")
    N_particles_black_hole = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of black hole particles in the snapshot.")
    N_particles_dark_matter = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of dark matter particles in the snapshot.")
    N_haloes = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of haloes.")
    N_haloes_top_level = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of haloes with no parent.")
    N_halo_children = NullableTypedAutoProperty[Collection[int]](NestedTypeShield(np.ndarray, np.int64),
                doc = "Number of direct children of each halo.")
    N_halo_decendants = NullableTypedAutoProperty[Collection[int]](NestedTypeShield(np.ndarray, np.int64),
                doc = "Total number of decendants of each halo.")
    N_halo_particles = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of particles in haloes.")
    N_halo_particles_gas = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of gas particles.")
    N_halo_particles_star = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of star particles.")
    N_halo_particles_black_hole = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of black hole particles.")
    N_halo_particles_dark_matter = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of dark matter particles.")
    N_particles_matched = TypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched particles in this snapshot.")
    N_particles_matched_gas = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched gas particles in this snapshot.")
    N_particles_matched_star = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched star particles in this snapshot.")
    N_particles_matched_black_hole = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched black_hole particles in this snapshot.")
    N_particles_matched_dark_matter = NullableTypedAutoProperty[int](TypeShield(int),
                doc = "Number of newly matched dark_matter particles in this snapshot.")
    particle_total_volume = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of all particles in the snapshot.")
    halo_particle_total_volume = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume particles in haloes in the snapshot.")
    particles_matched_total_volume = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched particles in the snapshot.")
    particles_matched_total_volume_gas = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched gas particles in the snapshot.")
    particles_matched_total_volume_star = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched star particles in the snapshot.")
    particles_matched_total_volume_black_hole = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched black hole particles in the snapshot.")
    particles_matched_total_volume_dark_matter = NullableTypedAutoProperty[unyt_quantity](TypeShield(unyt_quantity),
                doc = "Total volume of newly matched dark matter particles in the snapshot.")
    
    def __str__(self):
        return f"""\
redshift:                                {self.redshift}
snapshot file:                           {self.snapshot_filepath}
catalogue membership file:               {self.catalogue_membership_filepath}
catalogue properties file:               {self.catalogue_properties_filepath}
number of particles:                     {self.N_particles}
number of particles in haloes:           {self.N_halo_particles if self.N_halo_particles is not None else "---"}
number of particles newley matched:      {self.N_particles_matched}
total particle volume:                   {self.particle_total_volume if self.particle_total_volume is not None else "---"}
number of haloes:                        {self.N_haloes}
number of top level haloes:              {self.N_haloes_top_level if self.N_haloes_top_level is not None else "---"}
total volume of particles in haloes:     {self.halo_particle_total_volume if self.halo_particle_total_volume is not None else "---"}
total volume of particles newly matched: {self.particles_matched_total_volume if self.particles_matched_total_volume is not None else "---"}\
""" + (f"""
gas:
    number of particles:                     {self.N_particles_gas}
    number of particles in haloes:           {self.N_halo_particles_gas if self.N_halo_particles_gas is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_gas}
    total volume of particles newly matched: {self.particles_matched_total_volume_gas if self.particles_matched_total_volume_gas is not None else "---"}\
""" if self.N_particles_gas is not None        else "gas: -----------") + (f"""
stars:
    number of particles:                     {self.N_particles_star}
    number of particles in haloes:           {self.N_halo_particles_star if self.N_halo_particles_star is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_star}
    total volume of particles newly matched: {self.particles_matched_total_volume_star if self.particles_matched_total_volume_star is not None else "---"}\
""" if self.N_particles_star is not None       else "stars: ---------") + (f"""
black holes:
    number of particles:                     {self.N_particles_black_hole}
    number of particles in haloes:           {self.N_halo_particles_black_hole if self.N_halo_particles_black_hole is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_black_hole}
    total volume of particles newly matched: {self.particles_matched_total_volume_black_hole if self.particles_matched_total_volume_black_hole is not None else "---"}\
""" if self.N_particles_black_hole is not None else "black holes: ---") + (f"""
dark mater:
    number of particles:                     {self.N_particles_dark_matter}
    number of particles in haloes:           {self.N_halo_particles_dark_matter if self.N_halo_particles_dark_matter is not None else "---"}
    number of particles newly matched:       {self.N_particles_matched_dark_matter}
    total volume of particles newly matched: {self.particles_matched_total_volume_dark_matter if self.particles_matched_total_volume_dark_matter is not None else "---"}\
""" if self.N_particles_dark_matter is not None else "dark mater: ---")
    
    @staticmethod
    def initialise_partial(snapshot: SnapshotBase, catalogue: CatalogueBase, minimal: bool = False) -> "SnapshotStatsDataset":
        data =  SnapshotStatsDataset()

        data.snapshot_filepath = snapshot.filepath
        data.catalogue_membership_filepath = catalogue.membership_filepath
        data.catalogue_properties_filepath = catalogue.properties_filepath
        data.redshift = catalogue.z
        data.N_particles = sum([snapshot.number_of_particles(p) for p in ParticleType.get_all()])
        for part_type in ParticleType.get_all():
            setattr(data, f"N_particles_{part_type.name.replace(' ', '_')}", snapshot.number_of_particles(part_type))
        data.N_haloes = len(catalogue)
        if data.N_haloes > 0:
            data.N_halo_children = catalogue.number_of_children
            data.N_halo_decendants = catalogue.number_of_decendants

        if not minimal:
            data.particle_total_volume = sum(
                [
                    snapshot.get_volumes(p).to("Mpc**3").sum()
                    for p
                    in ParticleType.get_all()
                ],
                start = unyt_quantity(0.0, units = "Mpc**3")
            )

            data.N_haloes_top_level = int((catalogue.get_halo_parent_IDs() == -1).sum())

            data.N_halo_particles_gas = len(catalogue.get_particle_IDs(ParticleType.gas))
            data.N_halo_particles_star = len(catalogue.get_particle_IDs(ParticleType.star))
            data.N_halo_particles_black_hole = len(catalogue.get_particle_IDs(ParticleType.black_hole))
            data.N_halo_particles_dark_matter = len(catalogue.get_particle_IDs(ParticleType.dark_matter))

            data.N_halo_particles = sum([len(catalogue.get_particle_IDs(p)) for p in ParticleType.get_all()])
            data.halo_particle_total_volume = sum([ArrayReorder.create(snapshot.get_IDs(p), catalogue.get_particle_IDs(p))(snapshot.get_volumes(p).to("Mpc**3")).sum() for p in ParticleType.get_all()], start = unyt_quantity(0.0, units = "Mpc**3"))

        return data

    @staticmethod
    async def initialise_partial_async(snapshot: SnapshotBase, catalogue: CatalogueBase, minimal: bool = False) -> "SnapshotStatsDataset":#TODO:
        data =  SnapshotStatsDataset()

        data.snapshot_filepath = snapshot.filepath
        data.catalogue_membership_filepath = catalogue.membership_filepath
        data.catalogue_properties_filepath = catalogue.properties_filepath
        data.redshift = catalogue.z
        data.N_particles = sum([snapshot.number_of_particles(p) for p in ParticleType.get_all()])
        for part_type in ParticleType.get_all():
            setattr(data, f"N_particles_{part_type.name.replace(' ', '_')}", snapshot.number_of_particles(part_type))
        data.N_haloes = len(catalogue)
        if data.N_haloes > 0:
            data.N_halo_children = catalogue.number_of_children
            data.N_halo_decendants = catalogue.number_of_decendants

        if not minimal:

            # Async operation functions

            async def calc__particle_total_volume():
                data.particle_total_volume = sum(
                    [
                        (smoothing_lengths.to("Mpc")**3).sum()
                        for smoothing_lengths
                        in await asyncio.gather(*[snapshot.get_smoothing_lengths_async(p) for p in ParticleType.get_all()])
                    ],
                    start = unyt_quantity(0.0, units = "Mpc**3")
                ) * (np.pi * 4/3)

            async def calc__N_haloes_top_level():
                data.N_haloes_top_level = int((await catalogue.get_halo_parent_IDs_async() == -1).sum())

            async def calc__N_halo_particles_of_type(part_type: ParticleType):
                return len(await catalogue.get_particle_IDs_async(part_type))
            async def calc__N_halo_particles():
                (
                    data.N_halo_particles_gas,
                    data.N_halo_particles_star,
                    data.N_halo_particles_black_hole,
                    data.N_halo_particles_dark_matter,
                ) = await asyncio.gather(
                    calc__N_halo_particles_of_type(ParticleType.gas),
                    calc__N_halo_particles_of_type(ParticleType.star),
                    calc__N_halo_particles_of_type(ParticleType.black_hole),
                    calc__N_halo_particles_of_type(ParticleType.dark_matter)
                )

            # Run all async functions
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                future = loop.run_in_executor(pool,
                    asyncio.gather,
                    calc__particle_total_volume(),
                    calc__N_haloes_top_level(),
                    calc__N_halo_particles()
                )
                while not future.done(): pass
                future.result()

            data.N_halo_particles = sum([len(catalogue.get_particle_IDs(p)) for p in ParticleType.get_all()])
            data.halo_particle_total_volume = sum([(ArrayReorder.create(snapshot.get_IDs(p), catalogue.get_particle_IDs(p))(snapshot.get_smoothing_lengths(p).to("Mpc"))**3).sum() for p in ParticleType.get_all()], start = unyt_quantity(0.0, units = "Mpc**3")) * (np.pi * 4/3)

        return data

    @staticmethod
    def initialise_partial_alt(snapshot: SnapshotBase, catalogue: CatalogueBase, minimal: bool = False) -> "SnapshotStatsDataset":
        data =  SnapshotStatsDataset()

        data.snapshot_filepath = snapshot.filepath
        data.catalogue_membership_filepath = catalogue.membership_filepath
        data.catalogue_properties_filepath = catalogue.properties_filepath
        data.redshift = catalogue.z
        data.N_particles = sum([snapshot.number_of_particles(p) for p in ParticleType.get_all()])
        for part_type in ParticleType.get_all():
            setattr(data, f"N_particles_{part_type.name.replace(' ', '_')}", snapshot.number_of_particles(part_type))
        data.N_haloes = len(catalogue)
        if data.N_haloes > 0:
            data.N_halo_children = catalogue.number_of_children
            data.N_halo_decendants = catalogue.number_of_decendants

        if not minimal:

            # Async operation functions

            async def calc__particle_total_volume():
                data.particle_total_volume = sum(
                    [
                        (smoothing_lengths.to("Mpc")**3).sum()
                        for smoothing_lengths
                        in await asyncio.gather(*[snapshot.get_smoothing_lengths_async(p) for p in ParticleType.get_all()])
                    ],
                    start = unyt_quantity(0.0, units = "Mpc**3")
                ) * (np.pi * 4/3)

            async def calc__N_haloes_top_level():
                data.N_haloes_top_level = int((await catalogue.get_halo_parent_IDs_async() == -1).sum())

            async def calc__N_halo_particles_of_type(part_type: ParticleType):
                return len(await catalogue.get_particle_IDs_async(part_type))
            async def calc__N_halo_particles():
                (
                    data.N_halo_particles_gas,
                    data.N_halo_particles_star,
                    data.N_halo_particles_black_hole,
                    data.N_halo_particles_dark_matter,
                ) = await asyncio.gather(
                    calc__N_halo_particles_of_type(ParticleType.gas),
                    calc__N_halo_particles_of_type(ParticleType.star),
                    calc__N_halo_particles_of_type(ParticleType.black_hole),
                    calc__N_halo_particles_of_type(ParticleType.dark_matter)
                )

            # Run all async functions
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                future = loop.run_in_executor(pool,
                    asyncio.gather,
                    calc__particle_total_volume(),
                    calc__N_haloes_top_level(),
                    calc__N_halo_particles()
                )
                while not future.done(): pass
                future.result()

            data.N_halo_particles = sum([len(catalogue.get_particle_IDs(p)) for p in ParticleType.get_all()])
            data.halo_particle_total_volume = sum([(ArrayReorder.create(snapshot.get_IDs(p), catalogue.get_particle_IDs(p))(snapshot.get_smoothing_lengths(p).to("Mpc"))**3).sum() for p in ParticleType.get_all()], start = unyt_quantity(0.0, units = "Mpc**3")) * (np.pi * 4/3)

        return data

    @staticmethod
    def _read(file: h5.File) -> "SnapshotStatsDataset":
        pass

    @staticmethod
    def _write(file: h5.File, data: "SnapshotStatsDataset") -> h5.Group:
        pass



class ContraData_File(Struct):
    redshift:     float                    = TypedAutoProperty[float](TypeShield(float))
    _gas:         ParticleTypeDataset|None = NullableTypedAutoProperty[ParticleTypeDataset](TypeShield(ParticleTypeDataset))
    _stars:       ParticleTypeDataset|None = NullableTypedAutoProperty[ParticleTypeDataset](TypeShield(ParticleTypeDataset))
    _black_holes: ParticleTypeDataset|None = NullableTypedAutoProperty[ParticleTypeDataset](TypeShield(ParticleTypeDataset))
    _dark_matter: ParticleTypeDataset|None = NullableTypedAutoProperty[ParticleTypeDataset](TypeShield(ParticleTypeDataset))

    def __init__(self, file_number: str, redshift: float|None = None, dynamic_load: bool = False, reader: "OutputReader|DistributedOutputReader|None" = None, **kwargs) -> None:
        self.__number = file_number
        if redshift is not None:
            self.redshift = redshift
        self.__loaded = not dynamic_load
        if not self.__loaded:
            if reader is None:
                raise ValueError("No argument provided for parameter \"reader\". This is required when specifying dynamic loading.")
        self.__reader = reader
        super().__init__(**kwargs)

    @property
    def z(self) -> float:
        return self.redshift

    @property
    def gas(self) -> ParticleTypeDataset|None:
        self.ensure_loaded()
        return self._gas
    @gas.setter
    def gas(self, value: ParticleTypeDataset) -> None:
        self.ensure_loaded()
        self._gas = value
    @property
    def stars(self) -> ParticleTypeDataset|None:
        self.ensure_loaded()
        return self._stars
    @stars.setter
    def stars(self, value: ParticleTypeDataset) -> None:
        self.ensure_loaded()
        self._stars = value
    @property
    def black_holes(self) -> ParticleTypeDataset|None:
        self.ensure_loaded()
        return self._black_holes
    @black_holes.setter
    def black_holes(self, value: ParticleTypeDataset) -> None:
        self.ensure_loaded()
        self._black_holes = value
    @property
    def dark_matter(self) -> ParticleTypeDataset|None:
        self.ensure_loaded()
        return self._dark_matter
    @dark_matter.setter
    def dark_matter(self, value: ParticleTypeDataset) -> None:
        self.ensure_loaded()
        self._dark_matter = value

    def ensure_loaded(self) -> None:
        if not self.__loaded:
            self.__loaded = True
            with cast(OutputReader, self.__reader):
                self.redshift = cast(OutputReader|DistributedOutputReader, self.__reader).read_particle_type_dataset_redshift(cast(str, self.__number))
                try:
                    self._gas = cast(OutputReader|DistributedOutputReader, self.__reader).read_particle_type_dataset(ParticleType.gas, cast(str, self.__number))
                except KeyError: pass
                try:
                    self._stars = cast(OutputReader|DistributedOutputReader, self.__reader).read_particle_type_dataset(ParticleType.star, cast(str, self.__number))
                except KeyError: pass
                try:
                    self._black_holes = cast(OutputReader|DistributedOutputReader, self.__reader).read_particle_type_dataset(ParticleType.black_hole, cast(str, self.__number))
                except KeyError: pass
                try:
                    self._dark_matter = cast(OutputReader|DistributedOutputReader, self.__reader).read_particle_type_dataset(ParticleType.dark_matter, cast(str, self.__number))
                except KeyError: pass



class ContraData(Struct):
    """
    Data output by Contra.

    To read a file, call load() and pass the filepath.
    """

    def __init__(self, directory_altered: bool = False, *args, **kwargs) -> None:
        self.__sim_directory_changed = directory_altered
        super().__init__(*args, **kwargs)

    header: HeaderDataset = TypedAutoProperty[HeaderDataset](TypeShield(HeaderDataset),
                doc = "Header. Data about the Contra run and output file.")
    data: dict[str, ContraData_File] = TypedAutoProperty[dict[str, ContraData_File]](TypeShield(dict),#TODO: dict nested shield? (not cast!!!)
                doc = "Datasets by particle file.")
    snapshot_search_stats: list[SnapshotStatsDataset]|None = NullableTypedAutoProperty[list[SnapshotStatsDataset]](NestedTypeShield(list, SnapshotStatsDataset),
                doc = "Search stats for each searched snapshot.")
    simulation_files: FileTreeScraper_EAGLE|None = NullableTypedAutoProperty[FileTreeScraper_EAGLE](TypeShield(FileTreeScraper_EAGLE),
                doc = "File tree information and tools for loading simulation data files.")
    
    @property
    def file_numbers(self) -> tuple[str, ...]:
        keys = list(self.data.keys())
        keys.sort(key = float)
        return tuple(keys)
    
    def find_file_number_from_redshift(self, redshift: float) -> str:
        file_numbers = np.array(list(self.data.keys()), dtype = str)
        file_numbers = file_numbers[np.array([float(v) for v in file_numbers], dtype = float).argsort()]
        file_redshifts = np.array([self.data[file_number].redshift for file_number in file_numbers], dtype = float)
        prior_files_mask = file_redshifts >= redshift
        if prior_files_mask.sum() == 0:
            raise FileNotFoundError(f"Unable to find search data for a file with a redshift of (or exceding) {redshift}.\nThe first file has a redshift of {file_redshifts[0]}.")
        return str(file_numbers[prior_files_mask][-1])

    @staticmethod
    def _get_snapshot_static(filepath: str, sim_type: str):
        if sim_type == "SWIFT":
            return SnapshotSWIFT(filepath)
        elif sim_type == "EAGLE":
            return SnapshotEAGLE(filepath)
        else:
            raise NotImplementedError(f"\"ContraData._get_snapshot_static\" not implemented for source simulation type \"{sim_type}\".")

    def _get_snapshot(self, filepath: str) -> SnapshotBase:
        try:
            return ContraData._get_snapshot_static(filepath, self.header.simulation_type)
        except NotImplementedError:
            raise NotImplementedError(f"\"ContraData._get_snapshot\" not implemented for source simulation type \"{self.header.simulation_type}\".")

    @singledispatchmethod
    def get_snapshot(self, number: str) -> SnapshotBase:
        if self.simulation_files is None:
            raise RuntimeError("Unable to get snapshots as no file information present. `simulation_files` must be set.")
        return self.simulation_files.snapshots.get_by_number(number).load() if not self.header.uses_snipshots else self.simulation_files.snipshots.get_by_number(number).load()

    @get_snapshot.register(SnapshotStatsDataset)
    def _(self, stats: SnapshotStatsDataset) -> SnapshotBase:
        #return self._get_snapshot(stats.snapshot_filepath)
        raise NotImplementedError("TODO")#TODO:

    @get_snapshot.register(ParticleTypeDataset)
    def _(self, data: ParticleTypeDataset) -> SnapshotBase:
        return self.get_snapshot(data.file_number)

    @get_snapshot.register(float)
    def _(self, redshift: float) -> SnapshotBase:
        return self.get_snapshot(self.find_file_number_from_redshift(redshift))

    def _get_catalogue(self, membership_filepath: str|list[str], properties_filepath: str|list[str], snapshot: SnapshotBase) -> CatalogueBase:
        if self.header.simulation_type == "SWIFT":
            return CatalogueSOAP(membership_filepath, properties_filepath, snapshot)
        elif self.header.simulation_type == "EAGLE":
            return CatalogueSUBFIND(membership_filepath, properties_filepath, snapshot)
        else:
            raise NotImplementedError(f"\"ContraData._get_catalogue\" not implemented for source simulation type \"{self.header.simulation_type}\".")

    @singledispatchmethod
    def get_catalogue(self, number: str) -> CatalogueBase:
        if self.simulation_files is None:
            raise RuntimeError("Unable to get catalogues as no file information present. `simulation_files` must be set.")
        return self.simulation_files.catalogues.get_by_number(number).load() if not self.header.uses_snipshots else self.simulation_files.snipshot_catalogues.get_by_number(number).load()

    @get_catalogue.register(SnapshotStatsDataset)
    def _(self, stats: SnapshotStatsDataset) -> CatalogueBase:
        #return self._get_catalogue(stats.catalogue_membership_filepath, stats.catalogue_properties_filepath, self.get_snapshot(stats))
        raise NotImplementedError("TODO")#TODO:

#    @singledispatchmethod
#    def get_matched_particles_mask(self, redshift: Any, part_type: ParticleType):
#        """
#        Creates a numpy mask array for the target snapshot data (for the requested particle type)
#        that selects particles last found in a halo at the specified redshift.
#        """
#        try:
#            return self.get_matched_particles_mask(float(redshift), part_type)
#        except:
#            raise TypeError(f"Invalid argument type {type(redshift)} for first parameter of ContraData.get_matched_particles_mask")
#    @get_matched_particles_mask.register(float)
#    def _(self, redshift: float, part_type: ParticleType):
#        if part_type not in self.data:
#            raise KeyError(f"No search data avalible for {part_type.name} particles.")
#        return self.data[part_type].redshifts == redshift
#    @get_matched_particles_mask.register(SnapshotStatsDataset)
#    def _(self, stats: SnapshotStatsDataset, part_type: ParticleType):
#        return self.get_matched_particles_mask(stats.redshift, part_type)
#    @get_matched_particles_mask.register(SnapshotBase)
#    def _(self, snapshot: SnapshotBase, part_type: ParticleType):
#        return self.get_matched_particles_mask(snapshot.redshift, part_type)
#    @get_matched_particles_mask.register(CatalogueBase)
#    def _(self, catalogue: CatalogueBase, part_type: ParticleType):
#        return self.get_matched_particles_mask(catalogue.redshift, part_type)

#    @singledispatchmethod
#    def get_snapshot_indexes_of_matched_particles(self, *args, **kwargs):
#        """
#        Get a numpy array of indexes to the specified snapshot's specified particle type data that
#        returns particle data for the particles last found in haloes at that snapshot's redshift.
#
#        Call signitures:
#
#            file_number (str),               part_type (ParticleType)
#
#            stats (SnapshotStatsDataset),    part_type (ParticleType)
#
#            snapshot (SnapshotBase),         part_type (ParticleType)
#
#            catalogue (CatalogueBase),       part_type (ParticleType)
#        """
#        if len(args) > 0:
#            raise TypeError(f"Invalid argument type {type(args[0])} for first parameter of ContraData.get_snapshot_indexes_of_matched_particles")
#        else:
#            RuntimeError("This should be impossible! Please report!")
#    @get_snapshot_indexes_of_matched_particles.register(ParticleType)
#    def _(self, file_number: str, part_type: ParticleType):
#        if part_type not in self.data:
#            raise KeyError(f"No search data avalible for {part_type.name} particles.")
#        target_snap_mask = self.get_matched_particles_mask(self.get_target_snapshot().redshift, part_type)
#        return np.where(target_snap_mask)[0]
#    @get_snapshot_indexes_of_matched_particles.register(SnapshotStatsDataset)
#    def _(self, stats: SnapshotStatsDataset, part_type: ParticleType):
#        return self.get_snapshot_indexes_of_matched_particles(self.get_snapshot(stats), part_type)
#    @get_snapshot_indexes_of_matched_particles.register(SnapshotBase)
#    def _(self, snapshot: SnapshotBase, part_type: ParticleType):
#        if part_type not in self.data:
#            raise KeyError(f"No search data avalible for {part_type.name} particles.")
#        target_snap_mask = self.get_matched_particles_mask(snapshot.redshift, part_type)
#        target_snap_ids = self.get_target_snapshot().get_IDs(part_type)[target_snap_mask]
#        sorted_target_snap_ids_indexes = np.argsort(target_snap_ids)
#        return np.searchsorted(target_snap_ids, snapshot.get_IDs(part_type), sorter = sorted_target_snap_ids_indexes)[np.argsort(sorted_target_snap_ids_indexes)]
#    @get_snapshot_indexes_of_matched_particles.register(CatalogueBase)
#    def _(self, catalogue: CatalogueBase, part_type: ParticleType):
#        return self.get_snapshot_indexes_of_matched_particles(catalogue.snapshot, part_type)
    
    @staticmethod
    def load(filepath: str, include_stats: bool = True, alternate_simulation_data_directory: str|None = None) -> "ContraData":
        result = None
        reader = OutputReader(filepath)
        with reader:
            result = reader.read(include_stats, alternate_simulation_data_directory = alternate_simulation_data_directory)
        return result



class OutputWriter(object):
    """
    Writes datasets to an HDF5 file.
    """

    def __init__(self, filepath: str, overwrite = False, use_mpi: bool = False) -> None:
        self.__filepath = filepath
        self.__file: h5.File|None = None

        self.__parallel_write: bool = use_mpi

        # Create the file
        if not self.__parallel_write or MPI_Config.is_root:
            f = h5.File(self.__filepath, "w" if overwrite else "a")
            if overwrite or "SnapshotStats" not in f:
                f.create_group("SnapshotStats")
            f.close()

    @property
    def is_open(self):
        return self.__file is not None

    def open(self):
#        print(f"OPEN {self.__filepath}--------------------------------------------------------------------")
        if self.__parallel_write:
            self.__file = h5.File(self.__filepath, "a", driver = "mpio", comm = MPI_Config.comm)
        else:
            self.__file = h5.File(self.__filepath, "a")

    def close(self):
        if self.__parallel_write:
            mpi_barrier()
        Console.print_debug("Closing file.")
        self.__file.close()
        Console.print_debug("Closed.")
        if self.__parallel_write:
            mpi_barrier()
        self.__file = None
#        print(f"CLOSED {self.__filepath}--------------------------------------------------------------------")

    def __enter__(self) -> "OutputWriter":
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        self.close()

    def write_header(self, header: HeaderDataset, root: int|None = None) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        HeaderDataset._write(self.__file, header, self.__parallel_write, root)

    def write_particle_type_dataset(self, dataset: ParticleTypeDataset, overwrite: bool = False) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        
        if not overwrite and dataset.particle_type.common_hdf5_name in self.__file and dataset.file_number in self.__file[dataset.particle_type.common_hdf5_name]:
            raise KeyError("Output dataset already exists. Specify \"overwrite\" to allow overwriting of existing datasets.")
        
        ParticleTypeDataset._write(self.__file, dataset, self.__parallel_write)

    def write_snapshot_stats_dataset(self, index: int, stats: SnapshotStatsDataset, minimal = False) -> None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        d = self.__file["SnapshotStats"].create_group(str(index))
        d.attrs["SnapshotFilepath"] = stats.snapshot_filepath
        d.attrs["CatalogueMembershipFilepath"] = stats.catalogue_membership_filepath
        d.attrs["CataloguePropertiesFilepath"] = stats.catalogue_properties_filepath
        d.attrs["Redshift"] = stats.redshift
        d.attrs["NumberOfParticles"] = stats.N_particles
        d.attrs["NumberOfHaloes"] = stats.N_haloes
        if not minimal:
            d.attrs["NumberOfTopLevelHaloes"] = stats.N_haloes_top_level
            d.attrs["NumberOfHaloParticles"] = stats.N_halo_particles
        d.attrs["NumberOfMatchedParticles"] = stats.N_particles_matched
        if not minimal:
            d.attrs["VolumeOfParticles"] = stats.particle_total_volume.to("Mpc**3").value
            d.attrs["VolumeOfHaloParticles"] = stats.halo_particle_total_volume.to("Mpc**3").value
            d.attrs["VolumeOfMatchedParticles"] = stats.particles_matched_total_volume.to("Mpc**3").value

        d.create_dataset(name = "NumberOfTypedParticles", data = np.array([
            (stats.N_particles_gas         if stats.N_particles_gas         is not None else -1),
            (stats.N_particles_star        if stats.N_particles_star        is not None else -1),
            (stats.N_particles_black_hole  if stats.N_particles_black_hole  is not None else -1),
            (stats.N_particles_dark_matter if stats.N_particles_dark_matter is not None else -1)
        ], dtype = int))
        d.create_dataset(name = "NumberOfHaloTypedParticles", data = np.array([
            (stats.N_halo_particles_gas         if stats.N_halo_particles_gas         is not None else -1),
            (stats.N_halo_particles_star        if stats.N_halo_particles_star        is not None else -1),
            (stats.N_halo_particles_black_hole  if stats.N_halo_particles_black_hole  is not None else -1),
            (stats.N_halo_particles_dark_matter if stats.N_halo_particles_dark_matter is not None else -1)
        ], dtype = int))
        d.create_dataset(name = "NumberOfMatchedTypedParticles", data = np.array([
            (stats.N_particles_matched_gas         if stats.N_particles_matched_gas         is not None else -1),
            (stats.N_particles_matched_star        if stats.N_particles_matched_star        is not None else -1),
            (stats.N_particles_matched_black_hole  if stats.N_particles_matched_black_hole  is not None else -1),
            (stats.N_particles_matched_dark_matter if stats.N_particles_matched_dark_matter is not None else -1)
        ], dtype = int))
        d.create_dataset(name = "VolumeOfMatchedTypedParticles", data = np.array([
            (stats.particles_matched_total_volume_gas.to("Mpc**3").value         if stats.particles_matched_total_volume_gas         is not None else -1),
            (stats.particles_matched_total_volume_star.to("Mpc**3").value        if stats.particles_matched_total_volume_star        is not None else -1),
            (stats.particles_matched_total_volume_black_hole.to("Mpc**3").value  if stats.particles_matched_total_volume_black_hole  is not None else -1),
            (stats.particles_matched_total_volume_dark_matter.to("Mpc**3").value if stats.particles_matched_total_volume_dark_matter is not None else -1)
        ], dtype = float))
        if stats.N_haloes > 0:
            d.create_dataset(name = "HaloesNumberOfChildren", data = stats.N_halo_children)
            d.create_dataset(name = "HaloesNumberOfDecendants", data = stats.N_halo_decendants)

    def write(self, data: ContraData, allow_overwrite: bool = False) -> None:
        force_open = not self.is_open
        if force_open:
            self.open()
        self.write_header(data.header)
        for file_data in data.data.values():
            try:
                self.write_particle_type_dataset(file_data.gas, allow_overwrite)
            except: pass
            try:
                self.write_particle_type_dataset(file_data.stars, allow_overwrite)
            except: pass
            try:
                self.write_particle_type_dataset(file_data.black_holes, allow_overwrite)
            except: pass
            try:
                self.write_particle_type_dataset(file_data.dark_matter, allow_overwrite)
            except: pass
        if data.header.has_statistics:
            for i, stats in enumerate(data.snapshot_search_stats):
                self.write_snapshot_stats_dataset(i, stats)
        if force_open:
            self.close()



class OutputReader(object):
    """
    Reads saved Contra data.
    """

    def __init__(self, filepath: str, use_mpi: bool = False) -> None:
        self.__filepath = filepath
        self.__file: h5.File|None = None

        self.__parallel_read: bool = use_mpi

    @property
    def is_open(self):
        return self.__file is not None
    
    def open(self):
#        print(f"OPEN {self.__filepath}--------------------------------------------------------------------")
        self.__file = h5.File(self.__filepath, "r")

    def close(self):
        self.__file.close()
        self.__file = None
#        print(f"CLOSED {self.__filepath}--------------------------------------------------------------------")

    def __enter__(self) -> "OutputReader":
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        self.close()

    def read_header(self) -> HeaderDataset:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        return HeaderDataset._read(self.__file, self.__parallel_read)
    
    def is_checkpoint_avalible(self, part_type: ParticleType) -> bool:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if part_type.common_hdf5_name not in self.__file:
            return False
        if len(self.__file[part_type.common_hdf5_name]) == 0:
            return False
        return True

    def read_checkpoint(self, part_type: ParticleType) -> ParticleTypeDataset|None:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if not self.is_checkpoint_avalible(part_type):
            return None
        snap_numbers = list(self.__file[part_type.common_hdf5_name].keys())
        snap_numbers.sort(key = int)
        return self.read_particle_type_dataset(part_type, snap_numbers[-1])

    def _read_particle_type_dataset_keys(self, part_type: ParticleType) -> tuple[str, ...]:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        if part_type.common_hdf5_name not in self.__file:
            raise KeyError(f"Contra output file has no dataset \"{part_type.common_hdf5_name}\".")
        return tuple(self.__file[part_type.common_hdf5_name].keys())
    
    def read_particle_type_dataset_redshift(self, number: str) -> float:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        redshift: float|None = None
        for part_type in ParticleType.get_all():
            try:
                redshift = float(self.__file[part_type.common_hdf5_name][number].attrs["Redshift"])
                break
            except: pass
        if redshift is None:
            raise KeyError(f"Unable to find particle data for file number {number}.")
        return redshift

    def read_particle_type_dataset(self, part_type: ParticleType, number: str) -> ParticleTypeDataset:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        return ParticleTypeDataset._read(self.__file, part_type, number, self.__parallel_read)

    def read_snapshot_stats_dataset(self, index: int) -> SnapshotStatsDataset:
        if not self.is_open:
            raise IOError("File not open. Call open() first or use with statement.")
        n_snapshots = int(self.__file["Header"].attrs["NumberSearchedSnapshots"])
        true_index = index if index >= 0 else (n_snapshots - index)
        if true_index >= n_snapshots:
            raise IndexError(f"Index {index} out of bounds for number of snapshots avalible ({n_snapshots}).")
        d = self.__file[f"SnapshotStats/{index}"]
        s = SnapshotStatsDataset()

        s.snapshot_filepath = str(d.attrs["SnapshotFilepath"])
        s.catalogue_membership_filepath = str(d.attrs["CatalogueMembershipFilepath"])
        s.catalogue_properties_filepath = str(d.attrs["CataloguePropertiesFilepath"])
        s.redshift = float(d.attrs["Redshift"])
        s.N_particles = int(d.attrs["NumberOfParticles"])
        s.N_haloes = int(d.attrs["NumberOfHaloes"])
        if "NumberOfTopLevelHaloes" in d.attrs:
            s.N_haloes_top_level = int(d.attrs["NumberOfTopLevelHaloes"])
        if "NumberOfHaloParticles" in d.attrs:
            s.N_halo_particles = int(d.attrs["NumberOfHaloParticles"])
        if "NumberOfMatchedParticles" in d.attrs:
            s.N_particles_matched = int(d.attrs["NumberOfMatchedParticles"])
        if "VolumeOfParticles" in d.attrs:
            s.particle_total_volume = unyt_quantity(d.attrs["VolumeOfParticles"], units = "Mpc**3")
        if "VolumeOfHaloParticles" in d.attrs:
            s.halo_particle_total_volume = unyt_quantity(d.attrs["VolumeOfHaloParticles"], units = "Mpc**3")
        if "VolumeOfMatchedParticles" in d.attrs:
            s.particles_matched_total_volume = unyt_quantity(d.attrs["VolumeOfMatchedParticles"], units = "Mpc**3")

        N_particles__by_type = d["NumberOfTypedParticles"][:]
        if N_particles__by_type[0] > -1:
            s.N_particles_gas = int(N_particles__by_type[0])
        if N_particles__by_type[1] > -1:
            s.N_particles_star = int(N_particles__by_type[1])
        if N_particles__by_type[2] > -1:
            s.N_particles_black_hole = int(N_particles__by_type[2])
        if N_particles__by_type[3] > -1:
            s.N_particles_dark_matter = int(N_particles__by_type[3])

        N_halo_particles__by_type = d["NumberOfHaloTypedParticles"][:]
        if N_halo_particles__by_type[0] > -1:
            s.N_halo_particles_gas = int(N_halo_particles__by_type[0])
        if N_halo_particles__by_type[1] > -1:
            s.N_halo_particles_star = int(N_halo_particles__by_type[1])
        if N_halo_particles__by_type[2] > -1:
            s.N_halo_particles_black_hole = int(N_halo_particles__by_type[2])
        if N_halo_particles__by_type[3] > -1:
            s.N_halo_particles_dark_matter = int(N_halo_particles__by_type[3])

        N_particles_matched__by_type = d["NumberOfMatchedTypedParticles"][:]
        if N_particles_matched__by_type[0] > -1:
            s.N_particles_matched_gas = int(N_particles_matched__by_type[0])
        if N_particles_matched__by_type[1] > -1:
            s.N_particles_matched_star = int(N_particles_matched__by_type[1])
        if N_particles_matched__by_type[2] > -1:
            s.N_particles_matched_black_hole = int(N_particles_matched__by_type[2])
        if N_particles_matched__by_type[3] > -1:
            s.N_particles_matched_dark_matter = int(N_particles_matched__by_type[3])

        particles_matched_total_volume__by_type = d["VolumeOfMatchedTypedParticles"][:]
        if particles_matched_total_volume__by_type[0] > -1:
            s.particles_matched_total_volume_gas = unyt_quantity(particles_matched_total_volume__by_type[0], units = "Mpc**3")
        if particles_matched_total_volume__by_type[1] > -1:
            s.particles_matched_total_volume_star = unyt_quantity(particles_matched_total_volume__by_type[1], units = "Mpc**3")
        if particles_matched_total_volume__by_type[2] > -1:
            s.particles_matched_total_volume_black_hole = unyt_quantity(particles_matched_total_volume__by_type[2], units = "Mpc**3")
        if particles_matched_total_volume__by_type[3] > -1:
            s.particles_matched_total_volume_dark_matter = unyt_quantity(particles_matched_total_volume__by_type[3], units = "Mpc**3")

        if s.N_haloes > 0:
            s.N_halo_children = d["HaloesNumberOfChildren"][:]
            s.N_halo_decendants = d["HaloesNumberOfDecendants"][:]

        return s
    
    def read(self, include_stats: bool = True, alternate_simulation_data_directory: str|None = None) -> ContraData:
        force_open = not self.is_open
        if force_open:
            self.open()
        data_struct = ContraData(directory_altered = alternate_simulation_data_directory is not None)

        data_struct.header = self.read_header()

        data_struct.data = {}
        file_numbers: list[str] = []
        for part_type in ParticleType.get_all():
            try:
                file_numbers.extend(self._read_particle_type_dataset_keys(part_type))
            except: pass
        unique_file_numbers: tuple[str] = tuple(np.unique(file_numbers))
        for file_number in unique_file_numbers:
            redshift: float|None
            try:
                redshift = self.read_particle_type_dataset_redshift(file_number)
            except:
                Console.print_verbose_warning(f"Unable to locate redshift information in contra output for file number {file_number}.")
                redshift = None
            data_struct.data[file_number] = ContraData_File(file_number, redshift = redshift, dynamic_load = True, reader = self)

        if data_struct.header.has_statistics and include_stats:
            loaded_snap_stats = []
            for i in range(data_struct.header.N_searched_snapshots):
                loaded_snap_stats.append(self.read_snapshot_stats_dataset(i))
            data_struct.snapshot_search_stats = loaded_snap_stats
        else:
            data_struct.snapshot_search_stats = None

        if force_open:
            self.close()
        if alternate_simulation_data_directory is not None:
            data_struct.header.simulation_directory = alternate_simulation_data_directory

        #TODO:
        #data_struct.simulation_files = (FileTreeScraper_EAGLE if data_struct.header.sim_type == "EAGLE" else FileTreeScraper_SWIFT)(data_struct.header.simulation_directory)
        data_struct.simulation_files = FileTreeScraper_EAGLE(data_struct.header.simulation_directory)

        return data_struct



class DistributedOutputReader(object):
    """
    Reads saved Contra data from accross multiple files.
    """
    def __init__(self, filepath: str, map_to_mpi: bool = False) -> None:

        self.__map_to_mpi: bool = map_to_mpi

        self.__readers: list[OutputReader] = []
        filepath += ".{}"
        i: int = -1
        while os.path.exists(path := filepath.format(i := i+1)):
            self.__readers.append(OutputReader(path, use_mpi = False))
        self.__n_files = len(self.__readers)
        if self.__n_files == 0:
            raise FileNotFoundError(f"Unable to find a file named \"{filepath.format(0)}\".")
        if self.__map_to_mpi and self.__n_files != MPI_Config.comm_size:
            raise IndexError(f"Inconsistent number of files and MPI ranks. Unable to assign {self.__n_files} files onto {MPI_Config.comm_size} ranks.")
        
#        self.__is_open: bool = False

#    @property
#    def is_open(self):
#        return self.__is_open
#    
#    def open(self):
#        for reader in self.__readers:
#            reader.open()
#        self.__is_open = True
#
#    def close(self):
#        for reader in self.__readers:
#            reader.close()
#        self.__is_open = False
        
    def __enter__(self) -> "DistributedOutputReader":
#        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
#        self.close()
        pass

    def read_header(self) -> HeaderDataset:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")
        with self.__readers[0] as reader:
            return reader.read_header()

    def is_checkpoint_avalible(self, part_type: ParticleType) -> bool:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")
        checks: list[bool] = []
        for reader in self.__readers:
            with reader:
                checks.append(reader.is_checkpoint_avalible(part_type))
        return any(checks)

    def read_checkpoint(self, part_type: ParticleType) -> ParticleTypeDataset|None:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")

        if self.__map_to_mpi:
            with self.__readers[MPI_Config.rank] as reader:
                result = reader.read_checkpoint(part_type)

        else:
            if not self.is_checkpoint_avalible(part_type):
                return None

            #TODO: this may not work as the first dataset in each file could be from inconsistent snapshots?
            results: list[ParticleTypeDataset] = []
            for reader in self.__readers:
                with reader:
                    if reader.is_checkpoint_avalible(part_type):
                        results.append(reader.read_checkpoint(part_type))
            if len(results) == 0:
                return None
            result = ParticleTypeDataset()
            result.particle_type          = part_type
            result.target_redshift        = results[0].target_redshift
            result.file_number            = results[0].file_number
            result.redshifts              = np.concatenate([r.redshifts for r in results])
            result.halo_ids               = np.concatenate([r.halo_ids for r in results])
            result.halo_masses            = np.concatenate([r.halo_masses for r in results])
            result.halo_masses_scaled     = np.concatenate([r.halo_masses_scaled for r in results])
            result.positions_pre_ejection = np.concatenate([r.positions_pre_ejection for r in results])
            result.length                 = result.halo_ids.shape[0]
            result.length_this_rank       = result.halo_ids.shape[0]

        return result

    def _read_particle_type_dataset_keys(self, part_type: ParticleType) -> tuple[str, ...]:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")
        all_keys: list[str] = []
        for reader in self.__readers:
            try:
                with reader:
                    all_keys.extend(reader._read_particle_type_dataset_keys(part_type))
            except KeyError: pass
        return tuple(np.unique(all_keys))

    def read_particle_type_dataset_redshift(self, number: str) -> float:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")
        with self.__readers[0] as reader:
            return reader.read_particle_type_dataset_redshift(number)

    def read_particle_type_dataset(self, part_type: ParticleType, number: str) -> ParticleTypeDataset:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")
        if self.__map_to_mpi:
            with self.__readers[MPI_Config.rank] as reader:
                result = reader.read_particle_type_dataset(part_type, number)

        else:
            results: list[ParticleTypeDataset] = []
            for reader in self.__readers:
                with reader:
                    results.append(reader.read_particle_type_dataset(part_type, number))
            if len(results) == 0:
                raise RuntimeError("Internal error. Failed to load any data.")
            result = ParticleTypeDataset()
            result.particle_type          = part_type
            result.target_redshift        = results[0].target_redshift
            result.file_number            = number
            result.redshifts              = np.concatenate([r.redshifts for r in results])
            result.halo_ids               = np.concatenate([r.halo_ids for r in results])
            result.halo_masses            = np.concatenate([r.halo_masses for r in results])
            result.halo_masses_scaled     = np.concatenate([r.halo_masses_scaled for r in results])
            result.positions_pre_ejection = np.concatenate([r.positions_pre_ejection for r in results])
            result.length                 = result.halo_ids.shape[0]
            result.length_this_rank       = result.halo_ids.shape[0]

        return result

    def read_snapshot_stats_dataset(self, index: int) -> SnapshotStatsDataset:
#        if not self.is_open:
#            raise IOError("File not open. Call open() first or use with statement.")
        raise NotImplementedError()

    def read(self, include_stats: bool = True, alternate_simulation_data_directory: str|None = None) -> ContraData:
#        force_open = not self.is_open
#        if force_open:
#            self.open()
        data_struct = ContraData(directory_altered = alternate_simulation_data_directory is not None)

        data_struct.header = self.read_header()

        data_struct.data = {}
        file_numbers: list[str] = []
        for part_type in ParticleType.get_all():
            try:
                file_numbers.extend(self._read_particle_type_dataset_keys(part_type))
            except: pass
        unique_file_numbers: tuple[str] = tuple(np.unique(file_numbers))
        for file_number in unique_file_numbers:
            redshift: float|None
            try:
                redshift = self.read_particle_type_dataset_redshift(file_number)
            except:
                Console.print_verbose_warning(f"Unable to locate redshift information in contra output for file number {file_number}.")
                redshift = None
            data_struct.data[file_number] = ContraData_File(file_number, redshift = redshift, dynamic_load = True, reader = self)

        if data_struct.header.has_statistics and include_stats:
            loaded_snap_stats = []
            for i in range(data_struct.header.N_searched_snapshots):
                loaded_snap_stats.append(self.read_snapshot_stats_dataset(i))
            data_struct.snapshot_search_stats = loaded_snap_stats
        else:
            data_struct.snapshot_search_stats = None

#        if force_open:
#            self.close()
        if alternate_simulation_data_directory is not None:
            data_struct.header.simulation_directory = alternate_simulation_data_directory

        #TODO:
        #data_struct.simulation_files = (FileTreeScraper_EAGLE if data_struct.header.sim_type == "EAGLE" else FileTreeScraper_SWIFT)(data_struct.header.simulation_directory)
        data_struct.simulation_files = FileTreeScraper_EAGLE(data_struct.header.simulation_directory)

        return data_struct
